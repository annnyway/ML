{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGRR: Automatic Gapping Resolution for Russian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Подготовка данных (2 балла)\n",
    "Скачайте данные (train, dev, test). Для работы с таблицей используйте pandas. \n",
    "Посчитайте, сколько в каждом из файлов примеров из класса 0 и из класса 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import csv \n",
    "from string import punctuation\n",
    "from collections import Iterable\n",
    "from sklearn.linear_model import Perceptron\n",
    "from pymystem3 import Mystem\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "m = Mystem()\n",
    "punct = punctuation + '«»—…“”\"\"*–'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv',sep = '\\t', quoting=csv.QUOTE_NONE)\n",
    "dev = pd.read_csv('dev.csv',sep = '\\t', quoting=csv.QUOTE_NONE)\n",
    "test = pd.read_csv('test.csv',sep = '\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: \n",
      "0    10864\n",
      "1     5542\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"train: \")\n",
    "print(train[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev: \n",
      "0    2760\n",
      "1    1382\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"dev: \")\n",
    "print(dev[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: \n",
      "0    1365\n",
      "1     680\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"test: \")\n",
    "print(test[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    16384\n",
       "1     8302\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# объединим train и dev, будем тренировать на обоих\n",
    "train = pd.concat([train, dev], axis=0, ignore_index=True)\n",
    "train[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Анализ разметки (2 балла)\n",
    "Функция ниже преобразует оффсеты в колонках в числа и подсчитывает всевозможные сочетания оффсетов в датафрейме. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getOffsetVariants(df_):\n",
    "    \n",
    "    df = df_.loc[df_['class'] == 1]\n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    cV = []\n",
    "    cR1 = []\n",
    "    cR2 = []\n",
    "    V = []\n",
    "    R1 = []\n",
    "    R2 = []\n",
    "    \n",
    "    for i in df.index:\n",
    "        cV.append(len(str(df[\"cV\"][i]).split(\" \"))) if df[\"cV\"][i] != 0 else cV.append(0)\n",
    "        cR1.append(len(str(df[\"cR1\"][i]).split(\" \"))) if df[\"cR1\"][i] != 0 else cR1.append(0)\n",
    "        cR2.append(len(str(df[\"cR2\"][i]).split(\" \"))) if df[\"cR2\"][i] != 0 else cR2.append(0)\n",
    "        V.append(len(str(df[\"V\"][i]).split(\" \"))) if df[\"V\"][i] != 0 else V.append(0)\n",
    "        R1.append(len(str(df[\"R1\"][i]).split(\" \"))) if df[\"R1\"][i] != 0 else R1.append(0)\n",
    "        R2.append(len(str(df[\"R2\"][i]).split(\" \"))) if df[\"R2\"][i] != 0 else R2.append(0)\n",
    "        \n",
    "    assert len(cV) == len(cR1) == len(cR2) == len(V) == len(R1) == len(R2)\n",
    "    if not len(cV) == len(cR1) == len(cR2) == len(V) == len(R1) == len(R2):\n",
    "        raise AssertionError()\n",
    "        \n",
    "    df_cols = ['cV','cR1', 'cR2', 'V', 'R1', 'R2']\n",
    "    \n",
    "    table = pd.DataFrame({\"cV\": cV,\n",
    "                          \"cR1\": cR1,\n",
    "                          \"cR2\": cR2,\n",
    "                          \"V\": V, \n",
    "                          \"R1\": R1,  \n",
    "                          \"R2\": R2},  columns=df_cols)\n",
    "    \n",
    "    return table.groupby(df_cols).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py:4034: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cV  cR1  cR2  V  R1  R2\n",
       "1   0    1    1  1   1        1\n",
       "    1    0    1  1   0      131\n",
       "         1    1  1   1     7621\n",
       "              2  2   2      461\n",
       "              3  3   3       63\n",
       "              4  4   4       21\n",
       "              5  5   5        2\n",
       "              7  7   7        2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getOffsetVariants(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cV  cR1  cR2  V  R1  R2\n",
       "1   1    0    1  1   0      16\n",
       "              2  2   0       1\n",
       "         1    1  1   1     616\n",
       "              2  2   2      42\n",
       "              3  3   3       3\n",
       "              4  4   4       1\n",
       "              5  5   5       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getOffsetVariants(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что чаще всего в предложениях встречается один оффсет для каждой метки, но есть достаточно много предложений с двумя и более оффсетами для некоторых меток (V, R1, R2). Там, где нет метки R2 (=0), скорее всего эллипсис находится в конце предложения. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Конвертер разметки-1 (1 балл)\n",
    "\n",
    "Я выбрала IOB-разметку. Эта функция далась мне тяжело (заняла много времени), во-первых, из-за иногда неправильно отмеченных оффсетов в датасете, и, во-вторых, из-за ситуаций, когда между двумя словами, разделенными тире или дефисом, не было пробела. Мой код ниже неплохо справляется с данной задачей, но, к сожалению, уже ближе к дедлайну я обнаружила, что число тегов не всегда соответствует числу слов в очищенном от пунктуации предложении. Пришлось пожертвовать примерно 200-ми предложениями, чтобы подготовить данные для RNN без ошибок. Функция ниже. \n",
    "(Обратно из разметки можно перевести, используя индекс предложения). \n",
    "\n",
    "P.S. Буду благодарна, если в комментариях к домашней работе будут советы по более оптимальному решению, учитывающему все нюансы.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent): # функция токенизирует предложение и очищает от пунктуации\n",
    "    sent = sent.replace(\",\", \", \")\n",
    "    words = [word.rstrip(punct) for word in sent.split() if word.rstrip(punct)]\n",
    "    return words\n",
    "\n",
    "def flatten(list_): # функция делает из списков со вложенными списками список с одним уровнем вложенности\n",
    "     for item in list_:\n",
    "        if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "             for x in flatten(item):\n",
    "                    yield x\n",
    "        else:        \n",
    "             yield item\n",
    "\n",
    "def get_offsets(df, sent_indx, column_name): # функция извлекает слайсы предложений по оффсетам\n",
    "    offsets = str(df[column_name][sent_indx]).split(\" \")\n",
    "    return [j.split(\":\") for j in offsets if offsets != [\"nan\"]]\n",
    "\n",
    "def makeIOBtags(df): # непосредстенно сама функция разметки\n",
    "     \n",
    "    tagged_sents = {} # данные положим в словарь\n",
    "    \n",
    "    for indx in df.index:  \n",
    "        if df[\"class\"][indx] == 1:\n",
    "            sent = df[\"text\"][indx]\n",
    "        \n",
    "            cV = get_offsets(df, indx, \"cV\") # извлекаем слайсы\n",
    "            cR1 = get_offsets(df, indx, \"cR1\")\n",
    "            cR2 = get_offsets(df, indx, \"cR2\")\n",
    "            R1 = get_offsets(df, indx, \"R1\")\n",
    "            R2 = get_offsets(df, indx, \"R2\")\n",
    "        \n",
    "            cV_words = [sent[int(offset[0]):int(offset[1])] for offset in cV] # берем слова по слайсам согласно меткам\n",
    "            cR1_words = [sent[int(offset[0]):int(offset[1])] for offset in cR1]\n",
    "            cR2_words = [sent[int(offset[0]):int(offset[1])] for offset in cR2]\n",
    "            R1_words = [sent[int(offset[0]):int(offset[1])] for offset in R1]\n",
    "            R2_words = [sent[int(offset[0]):int(offset[1])] for offset in R2]\n",
    "        \n",
    "            for i, chunk in enumerate(cV_words):\n",
    "                sent = sent.replace(chunk, \" cV\" + \"_\" + str(i)) # заменяем слайсы предложений метками и номером оффсета\n",
    "            for i, chunk in enumerate(cR1_words):\n",
    "                sent = sent.replace(chunk, \" cR1\" + \"_\" + str(i))\n",
    "            for i, chunk in enumerate(cR2_words):\n",
    "                sent = sent.replace(chunk, \" cR2\" + \"_\" + str(i))\n",
    "            for i, chunk in enumerate(R1_words):\n",
    "                sent = sent.replace(chunk, \" R1\" + \"_\" + str(i))\n",
    "            for i, chunk in enumerate(R2_words):\n",
    "                sent = sent.replace(chunk, \" R2\" + \"_\" + str(i))\n",
    "                        \n",
    "            t_sent = tokenize(sent) # токенизируем предложение\n",
    "            labels = (\"cV\", \"cR1\", \"cR2\", \"R1\", \"R2\")          \n",
    "            try:\n",
    "                for i, word in enumerate(t_sent): # для каждой метки добавляем IOB-теги\n",
    "                    if not word.startswith(labels):\n",
    "                        t_sent[i] = \"O\"\n",
    "                    elif word.startswith(\"cV\"):\n",
    "                        tokenized_cV_words = [tokenize(i) for i in cV_words]\n",
    "                        chunk_number = int(word.split(\"_\")[1][0]) # учитываем номер оффсета, чтобы не ошибиться с числом слов в нем\n",
    "                        chunk_length = len(tokenized_cV_words[chunk_number])\n",
    "                        if chunk_length == 1:\n",
    "                            t_sent[i] = \"B-cV\"\n",
    "                        elif chunk_length > 1:\n",
    "                            t_sent[i] = ([\"B-cV\"] + [\"I-cV\" for i in range(chunk_length-1)])\n",
    "                    elif word.startswith(\"cR1\"):\n",
    "                        tokenized_cR1_words = [tokenize(i) for i in cR1_words]\n",
    "                        chunk_number = int(word.split(\"_\")[1][0])\n",
    "                        chunk_length = len(tokenized_cR1_words[chunk_number])\n",
    "                        if chunk_length == 1:\n",
    "                            t_sent[i] = \"B-cR1\"\n",
    "                        elif chunk_length > 1:\n",
    "                            t_sent[i] = ([\"B-cR1\"] + [\"I-cR1\" for i in range(chunk_length-1)])\n",
    "                    elif word.startswith(\"cR2\"):\n",
    "                        tokenized_cR2_words = [tokenize(i) for i in cR2_words]\n",
    "                        chunk_number = int(word.split(\"_\")[1][0])\n",
    "                        chunk_length = len(tokenized_cR2_words[chunk_number])\n",
    "                        if chunk_length == 1:\n",
    "                            t_sent[i] = \"B-cR2\"\n",
    "                        elif chunk_length > 1:\n",
    "                            t_sent[i] = ([\"B-cR2\"] + [\"I-cR2\" for i in range(chunk_length-1)])\n",
    "                    elif word.startswith(\"R1\"):\n",
    "                        tokenized_R1_words = [tokenize(i) for i in R1_words]\n",
    "                        chunk_number = int(word.split(\"_\")[1][0])\n",
    "                        chunk_length = len(tokenized_R1_words[chunk_number])\n",
    "                        if chunk_length == 1:\n",
    "                            t_sent[i] = \"B-R1\"\n",
    "                        elif chunk_length > 1:\n",
    "                            t_sent[i] = ([\"B-R1\"] + [\"I-R1\" for i in range(chunk_length-1)])\n",
    "                    elif word.startswith(\"R2\"):\n",
    "                        tokenized_R2_words = [tokenize(i) for i in R2_words]\n",
    "                        chunk_number = int(word.split(\"_\")[1][0])\n",
    "                        chunk_length = len(tokenized_R2_words[chunk_number])\n",
    "                        if chunk_length == 1:\n",
    "                            t_sent[i] = \"B-R2\"\n",
    "                        elif chunk_length > 1:\n",
    "                            t_sent[i] = ([\"B-R2\"] + [\"I-R2\" for i in range(chunk_length-1)])\n",
    "            except ValueError and IndexError:\n",
    "                continue\n",
    "            \n",
    "            tags = list(flatten(t_sent)) # выравниваем список\n",
    "            tagged_sents[indx] = tags # добавляем в словарь и сохраняем номер предложения\n",
    "        \n",
    "        elif df[\"class\"][indx] == 0:  \n",
    "            sent = train[\"text\"][indx]\n",
    "            sent_length = len(tokenize(sent))\n",
    "            tags = [\"O\" for i in range(sent_length)]\n",
    "            tagged_sents[indx] = tags\n",
    "                        \n",
    "    return tagged_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_for_train = makeIOBtags(train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-cR1', 'I-cR1', 'B-cV', 'O', 'B-cR2', 'O', 'O', 'B-R1', 'B-R2', 'I-R2']\n",
      "Работа с двухбайтовыми наборами символов — просто кошмар для программиста, так как часть их состоит из одного байта, а часть — из двух.\n"
     ]
    }
   ],
   "source": [
    "print(tags_for_train[1], train.text[1], sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Функция будет использоваться ниже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Конвертер разметки-2 (1 балл)\n",
    "\n",
    "Простая и понятная функция. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsets2brackets(df):\n",
    "    sents = {}\n",
    "    for indx in df.index:  \n",
    "        if df[\"class\"][indx] == 1:\n",
    "            sent = df[\"text\"][indx]\n",
    "            \n",
    "            cV = get_offsets(df, indx, \"cV\")\n",
    "            cR1 = get_offsets(df, indx, \"cR1\")\n",
    "            cR2 = get_offsets(df, indx, \"cR2\")\n",
    "            V = get_offsets(df, indx, \"V\") # не забываем про сам эллипсис\n",
    "            R1 = get_offsets(df, indx, \"R1\")\n",
    "            R2 = get_offsets(df, indx, \"R2\")\n",
    "            \n",
    "            cV_words = [sent[int(offset[0]):int(offset[1])] for offset in cV]\n",
    "            cR1_words = [sent[int(offset[0]):int(offset[1])] for offset in cR1]\n",
    "            cR2_words = [sent[int(offset[0]):int(offset[1])] for offset in cR2]\n",
    "            R1_words = [sent[int(offset[0]):int(offset[1])] for offset in R1]\n",
    "            R2_words = [sent[int(offset[0]):int(offset[1])] for offset in R2]\n",
    "            \n",
    "            for offset in V:\n",
    "                sent = sent[:int(offset[0])] + \"V[] \" + sent[int(offset[0]):]\n",
    "             \n",
    "            for chunk in cV_words:\n",
    "                new_chunk = \"cV[ \" + chunk + \" cV]\"\n",
    "                sent = sent.replace(chunk, new_chunk)\n",
    "                \n",
    "            for chunk in cR1_words:\n",
    "                new_chunk = \"cR1[ \" + chunk + \" cR1]\"\n",
    "                sent = sent.replace(chunk, new_chunk)\n",
    "                \n",
    "            for chunk in cR2_words:\n",
    "                new_chunk = \"cR2[ \" + chunk + \" cR2]\"\n",
    "                sent = sent.replace(chunk, new_chunk)\n",
    "            \n",
    "            for chunk in R1_words:\n",
    "                new_chunk = \"R1[ \" + chunk + \" R1]\"\n",
    "                sent = sent.replace(chunk, new_chunk)\n",
    "            \n",
    "            for chunk in R2_words:\n",
    "                new_chunk = \"R2[ \" + chunk + \" R2]\"\n",
    "                sent = sent.replace(chunk, new_chunk)\n",
    "                            \n",
    "            sents[indx] = sent\n",
    "            \n",
    "    return sents\n",
    "            \n",
    "sents = offsets2brackets(train[156:157])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cR1[ Я cR1] и так этого года cV[ ждала cV] наверное cR2[ 9 cR2] лет а R1[ мама R1] V[] R2[ 11 R2].'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[156]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Бинарная классификация (6 баллов (+2))\n",
    "Постройте простую полносвязную сеть, берущую на вход все предложение и на выходе предсказывающую наличие или отсутствие гэппинга в предложении. Признаки: использование эмбеддингов +1 балл, использование pos-тэгов +1 балл. \n",
    "\n",
    "##### В данном задании я много экспериментировала, в итоге остановилась на конфигурации многослойного перцептрона на keras. Использовала и эмбеддинги, и POS-теги. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим предложения в виде векторов с эмбеддингами и частями речи для каждого слова. Для начала лемматизируем и соберем частеречные теги. \n",
    "\n",
    "### 5.1 лемматизируем данные и получим POS-теги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analyzeWithPunct(sents): # лемматизируем с сохранением пунктуации\n",
    "    all_lemmas = []\n",
    "    all_tags = []\n",
    "    for sent in sents:\n",
    "        analysis = m.analyze(sent)\n",
    "        lemmas = []\n",
    "        tags = []\n",
    "        for word in analysis:\n",
    "            try:\n",
    "                pos = word[\"analysis\"][0][\"gr\"].split(\",\")[0].split(\"=\")[0]\n",
    "                lemma = word[\"analysis\"][0][\"lex\"]\n",
    "                tags.append(pos)\n",
    "                lemmas.append(lemma)\n",
    "            except:\n",
    "                symbol = word[\"text\"]\n",
    "                if symbol not in \" \\n\": \n",
    "                    lemma = symbol.strip() # сохраняем пунктуацию (и другой шум) как отдельный токен\n",
    "                    if symbol.strip() == \",\":\n",
    "                        pos = \"COMMA\"\n",
    "                    elif symbol.strip() in \"—-\":\n",
    "                        pos = \"DASH\"\n",
    "                    else:\n",
    "                        pos = \"PUNCT\"\n",
    "                    tags.append(pos) \n",
    "                    lemmas.append(lemma)\n",
    "        all_lemmas.append(lemmas)\n",
    "        all_tags.append(tags)\n",
    "    return all_lemmas, all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lemmas, train_pos = analyzeWithPunct([i for i in train.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['работа', 'с', 'двухбайтовый', 'набор', 'символ', '—', 'просто', 'кошмар', 'для', 'программист', ',', 'так', 'как', 'часть', 'они', 'состоять', 'из', 'один', 'байт', ',', 'а', 'часть', '—', 'из', 'два', '.'] ['S', 'PR', 'A', 'S', 'S', 'DASH', 'PART', 'S', 'PR', 'S', 'COMMA', 'ADVPRO', 'ADVPRO', 'S', 'SPRO', 'V', 'PR', 'ANUM', 'S', 'COMMA', 'CONJ', 'S', 'DASH', 'PR', 'NUM', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "print(train_lemmas[1], train_pos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lemmas, test_pos = analyzeWithPunct([i for i in test.text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Чтобы создать входной вектор определенной длины, узнаем максимальное число слов в предложениях train и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.8590401983354\n"
     ]
    }
   ],
   "source": [
    "lemmas = train_lemmas + test_lemmas\n",
    "len_ = 0\n",
    "counter = 0\n",
    "for i in lemmas:\n",
    "    counter += 1\n",
    "    len_ += len(i)\n",
    "avr_len = len_/counter \n",
    "print(avr_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, средняя длина предложений - 20. Мы будем считать, что в среднем предложении 25 слов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Нормализуем POS-теги и преобразуем в one-hot вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим словарь, который нормализует число POS-тегов до 25 на предложение \n",
    "# если в предложени более 25 слов, оно обрезается сначала, так как эллипсис чаще встречается в конце предложения\n",
    "# также данная функция приводит теги Mystem к более общему виду: например, прилагательное-числительное у нас будет просто прилагательным\n",
    "def tag2vec(tags_list):\n",
    "    dictionary = {\"V\":\"VERB\", \"S\":\"NOUN\", \"SPRO\":\"NOUN\", \"A\":\"ADJ\", \"ANUM\":\"ADJ\", \"APRO\":\"ADJ\", \n",
    "                  \"ADV\":\"ADV\", \"ADVPRO\":\"ADV\", \"CONJ\":\"CONJ\", \"PR\":\"PR\", \"PART\":\"PART\", \"INTJ\":\"INTJ\", \n",
    "                  \"NUM\":\"NUM\", \"COM\":\"COM\", \"COMMA\":\"COMMA\", \"DASH\":\"DASH\", \"PUNCT\":\"PUNCT\"}\n",
    "    tags_vec = [[dictionary[tag] for tag in sent] for sent in tags_list]\n",
    "    for i, vec in enumerate(tags_vec):\n",
    "        if len(vec) > 25:\n",
    "            tags_vec[i] = vec[len(vec) - 25 : ]\n",
    "        elif len(vec) < 25:\n",
    "            tags_vec[i] = vec + [\"0\" for i in range(25 - len(vec))]\n",
    "    return tags_vec\n",
    "\n",
    "train_tags_vec = tag2vec(train_pos)\n",
    "test_tags_vec = tag2vec(test_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADJ', 'NOUN', 'VERB', 'VERB', 'PUNCT', 'NOUN', 'NOUN', 'PUNCT', 'NOUN', 'NOUN', 'COMMA', 'CONJ', 'ADJ', 'PUNCT', 'PUNCT', 'NOUN', 'ADJ', 'NOUN', 'PUNCT', '0', '0', '0', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "print(train_tags_vec[8]) # получается вот так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      VERB\n",
       "1      NOUN\n",
       "2       ADJ\n",
       "3       ADV\n",
       "4      CONJ\n",
       "5        PR\n",
       "6      PART\n",
       "7      INTJ\n",
       "8       NUM\n",
       "9       COM\n",
       "10    COMMA\n",
       "11     DASH\n",
       "12    PUNCT\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# переведем  pos-теги в one-hot вектора\n",
    "pos_list = [\"VERB\", \"NOUN\", \"ADJ\", \"ADV\", \"CONJ\", \"PR\", \"PART\", \"INTJ\", \"NUM\", \"COM\", \"COMMA\", \"DASH\", \"PUNCT\"]\n",
    "pos_list = pd.DataFrame(pos_list)\n",
    "pos_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADV</th>\n",
       "      <th>COM</th>\n",
       "      <th>COMMA</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>DASH</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PART</th>\n",
       "      <th>PR</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>VERB</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ADJ  ADV  COM  COMMA  CONJ  DASH  INTJ  NOUN  NUM  PART  PR  PUNCT  VERB  \\\n",
       "0     0    0    0      0     0     0     0     0    0     0   0      0     1   \n",
       "1     0    0    0      0     0     0     0     1    0     0   0      0     0   \n",
       "2     1    0    0      0     0     0     0     0    0     0   0      0     0   \n",
       "3     0    1    0      0     0     0     0     0    0     0   0      0     0   \n",
       "4     0    0    0      0     1     0     0     0    0     0   0      0     0   \n",
       "5     0    0    0      0     0     0     0     0    0     0   1      0     0   \n",
       "6     0    0    0      0     0     0     0     0    0     1   0      0     0   \n",
       "7     0    0    0      0     0     0     1     0    0     0   0      0     0   \n",
       "8     0    0    0      0     0     0     0     0    1     0   0      0     0   \n",
       "9     0    0    1      0     0     0     0     0    0     0   0      0     0   \n",
       "10    0    0    0      1     0     0     0     0    0     0   0      0     0   \n",
       "11    0    0    0      0     0     1     0     0    0     0   0      0     0   \n",
       "12    0    0    0      0     0     0     0     0    0     0   0      1     0   \n",
       "\n",
       "    0  \n",
       "0   0  \n",
       "1   0  \n",
       "2   0  \n",
       "3   0  \n",
       "4   0  \n",
       "5   0  \n",
       "6   0  \n",
       "7   0  \n",
       "8   0  \n",
       "9   0  \n",
       "10  0  \n",
       "11  0  \n",
       "12  0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_pos = pd.get_dummies(pos_list[0])\n",
    "one_hot_pos[\"0\"] = [0 for i in range(13)]\n",
    "one_hot_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tags_vec = [[one_hot_pos[i].tolist() for i in tags] for tags in test_tags_vec]\n",
    "train_tags_vec = [[one_hot_pos[i].tolist() for i in tags] for tags in train_tags_vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Эмбеддинги и окончательные вектора признаков\n",
    "Я беру эмбеддинги из модели fastText, предобученной на собранном мной корпусе статей Новой газеты за 2015-2018 годы из рублик \"Культура\" и \"Общество\". См. \"lemmatized_novaya_gazeta.txt\". Обучение велось на леммах (4325550 токенов)), без предлогов, местоимений и других стоп-слов. Размерность эмбеддингов в моей модели - 50.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "model2 = FastText.load_fasttext_format('fasttext/model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция объединяет признаки слов для предложения\n",
    "\n",
    "def get_vector(sent, tags, model):\n",
    "    matrix = np.zeros((25,50+13)) # размерность вектора предложения = 1575 (50 - эмбеддинг, 13 - POS-tag)\n",
    "    if len(sent) > 25:\n",
    "        sent = sent[len(sent) - 25 : ]\n",
    "    for i, lemma in enumerate(sent):\n",
    "        if lemma in model2.wv:\n",
    "            vec = np.append(model2.wv[lemma], tags[i]) # эмбеддинг и POS-тег для каждого слова\n",
    "            matrix[i] = vec\n",
    "        else:\n",
    "            pass\n",
    "    vector = np.concatenate(matrix)\n",
    "    return vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = [] # получаем вектора для train\n",
    "for i in range(len(train_lemmas)):\n",
    "    sent_vec = get_vector(train_lemmas[i], train_tags_vec[i], model2)\n",
    "    train_vec.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = [] # получаем вектора для test\n",
    "for i in range(len(test_lemmas)):\n",
    "    sent_vec = get_vector(test_lemmas[i], test_tags_vec[i], model2)\n",
    "    test_vec.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [train_set[\"class\"][i] for i in train_set.index] # и классы\n",
    "test_labels = [test[\"class\"][i] for i in test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные для модели почти готовы!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Модель на Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "import keras_metrics as km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.stack(train_vec, axis=0) # преобразуем данные так, чтобы Keras прочел\n",
    "x_test = np.stack(test_vec, axis=0)\n",
    "y_train = np.asarray(train_labels)\n",
    "y_test = np.asarray(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_82\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_195 (Dense)            (None, 300)               472800    \n",
      "_________________________________________________________________\n",
      "activation_195 (Activation)  (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_114 (Dropout)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_196 (Dense)            (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "activation_196 (Activation)  (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_115 (Dropout)        (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_197 (Dense)            (None, 1)                 201       \n",
      "_________________________________________________________________\n",
      "activation_197 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 533,201\n",
      "Trainable params: 533,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# проектируем модель\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(300, input_dim=1575), # на вход идет вектор размерностью 1575 (т.е. 50+13 (эмбеддинг + pos-тег на слово) * 25)\n",
    "    Activation('relu'), # 3 скрытых слоя\n",
    "    Dropout(0.5),\n",
    "    Dense(200),\n",
    "    Activation(\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(1),\n",
    "    Activation('sigmoid') \n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20543 samples, validate on 2045 samples\n",
      "Epoch 1/5\n",
      "20543/20543 [==============================] - 8s 393us/step - loss: 0.0775 - precision: 0.9616 - recall: 0.9785 - val_loss: 0.8116 - val_precision: 0.6546 - val_recall: 0.7721\n",
      "Epoch 2/5\n",
      "20543/20543 [==============================] - 8s 403us/step - loss: 0.0795 - precision: 0.9660 - recall: 0.9763 - val_loss: 0.8654 - val_precision: 0.6335 - val_recall: 0.8059\n",
      "Epoch 3/5\n",
      "20543/20543 [==============================] - 8s 396us/step - loss: 0.0740 - precision: 0.9632 - recall: 0.9760 - val_loss: 0.8809 - val_precision: 0.6453 - val_recall: 0.7706\n",
      "Epoch 4/5\n",
      "20543/20543 [==============================] - 8s 383us/step - loss: 0.0714 - precision: 0.9693 - recall: 0.9767 - val_loss: 0.8746 - val_precision: 0.6514 - val_recall: 0.7750\n",
      "Epoch 5/5\n",
      "20543/20543 [==============================] - 8s 371us/step - loss: 0.0707 - precision: 0.9682 - recall: 0.9764 - val_loss: 0.9692 - val_precision: 0.6349 - val_recall: 0.8029\n"
     ]
    }
   ],
   "source": [
    "# компилируем модель. В качестве оптимизатора используем Adam \n",
    "# и в качестве функции потерь - кроссэнтропию для бинарной классификации\n",
    "\n",
    "model.compile( \n",
    " optimizer = \"adam\",\n",
    " loss = \"binary_crossentropy\", \n",
    " metrics = [km.binary_precision(), km.binary_recall()]\n",
    ")\n",
    "\n",
    "results = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs = 5, # вроде бы 5 эпох достаточно\n",
    "    batch_size = 50,\n",
    "    validation_data = (x_test, y_test), # валидируем на тесте для удобства\n",
    "    class_weight = {0:1., 1:2.} # балансируем классы\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7090909089988193"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test)\n",
    "score[1] * score[2] * 2 / (score[1] + score[2])\n",
    "# получаем такую F-меру "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37975326630030815, 0.7053254437035118, 0.8612716761761169]\n",
      "[0.3674013987654897, 0.7431551498379199, 0.8236994218462863]\n",
      "[0.35165943464223487, 0.74617346929258, 0.8453757224211885]\n",
      "[0.31796797029039575, 0.7457627117741208, 0.8901734102759865]\n",
      "[0.39029992639496125, 0.7144654087151616, 0.8208092484362993]\n",
      "[0.41968430570003007, 0.6681081080358802, 0.8930635836859735]\n",
      "[0.3255780176735667, 0.7561929594841992, 0.8381502888962211]\n",
      "[0.3612417893841529, 0.7703281026005239, 0.7803468206964818]\n",
      "[0.3792059494231437, 0.7503467405339326, 0.7817919074014752]\n",
      "[0.3695523513565398, 0.7496617049053232, 0.8017366134874477]\n",
      "0.7800445754687704\n"
     ]
    }
   ],
   "source": [
    "# пробуем ручную кросс-валидацию\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "scores = []\n",
    "\n",
    "for train, test in kfold.split(x_train, y_train):\n",
    "    model2 = Sequential([\n",
    "        Dense(300, input_dim=1575),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(200),\n",
    "        Activation(\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(1),\n",
    "        Activation('sigmoid')\n",
    "    ])\n",
    "    model2.compile(\n",
    "     optimizer = \"adam\",\n",
    "     loss = \"binary_crossentropy\",\n",
    "     metrics = [km.binary_precision(), km.binary_recall()]\n",
    "    )\n",
    "    results = model2.fit(\n",
    "        x_train[train], y_train[train],\n",
    "        verbose = 0,\n",
    "        epochs = 5,\n",
    "        batch_size = 50,\n",
    "        class_weight = {0:1, 1:2}\n",
    "    )\n",
    "    score = model2.evaluate(x_train[test], y_train[test], verbose=0)\n",
    "    print(score)\n",
    "    scores.append(score[1] * score[2] * 2 / (score[1] + score[2]))\n",
    "    \n",
    "    \n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2045/2045 [==============================] - 1s 271us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5330053753258255, 0.6067538125700703, 0.8191176469383651]"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model2.evaluate(x_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.697121401664941"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score[1] * score[2] * 2 / (score[1] + score[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если я сделала все правильно, то с кросс-валидацией мы получили почти такие же результаты. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Оценка результатов бинарной классификации (1 балл)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(x_test) # предсказываем\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame([test.text[i] for i in test.index], columns = [\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df[\"class\"] = predictions # размечаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Изобретение относится к судостроению и касаетс...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Эти состояния называют фазами воды, а превраще...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>И должен ни единой долькой  Не отступаться от ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Он потребовал обеспечить полное осуществление ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>По мнению местного пастора Элла Эбанкса, запре...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  class\n",
       "0  Изобретение относится к судостроению и касаетс...      0\n",
       "1  Эти состояния называют фазами воды, а превраще...      1\n",
       "2  И должен ни единой долькой  Не отступаться от ...      0\n",
       "3  Он потребовал обеспечить полное осуществление ...      0\n",
       "4  По мнению местного пастора Элла Эбанкса, запре...      0"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(\"output.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скрипт agrr_metrics.py выдал следующую оценку:\n",
    "````\n",
    "Binary classification quality (f1-score): 0.7131258457374832\n",
    "Other metrics: \n",
    " Precision: 0.6604010025062657\n",
    " Recall: 0.775\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-мера составила 0.71. Надеюсь, это неплохой результат, учитывая то, что это обычная сеть, и то, что здесь не использовался синтаксис в качестве признаков.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Классификация последовательности (7 баллов) \n",
    "Задача почти доделана, будет чуть позже! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Оценка результатов полной разметки (1 балл)\n",
    "Обученной сеткой разметьте тестовые данные, с помощью готового скрипта agrr_metrics.py оцените полученный результат.\n",
    "Пример вызова из командной строки:\n",
    "> python3 agrr_metrics.py test.csv output.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Анализ результатов (до 4 баллов)\n",
    "Проведите анализ ошибок. Посмотрите на fn, fp примеры, приведите несколько таких предложений в скобочной записи (если вы решали задачу классификации последовательности). Можно ли попытаться обобщить получаемые ошибки?\n",
    "\n",
    "to be continued... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
