{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGRR: Automatic Gapping Resolution for Russian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Подготовка данных (2 балла)\n",
    "Скачайте данные (train, dev, test). Для работы с таблицей используйте pandas. \n",
    "Посчитайте, сколько в каждом из файлов примеров из класса 0 и из класса 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import csv \n",
    "from string import punctuation\n",
    "from collections import Iterable\n",
    "from sklearn.linear_model import Perceptron\n",
    "from pymystem3 import Mystem\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "m = Mystem()\n",
    "punct = punctuation + '«»—…“”\"\"*–'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv',sep = '\\t', quoting=csv.QUOTE_NONE)\n",
    "dev = pd.read_csv('dev.csv',sep = '\\t', quoting=csv.QUOTE_NONE)\n",
    "test = pd.read_csv('test.csv',sep = '\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: \n",
      "0    10864\n",
      "1     5542\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"train: \")\n",
    "print(train[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev: \n",
      "0    2760\n",
      "1    1382\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"dev: \")\n",
    "print(dev[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: \n",
      "0    1365\n",
      "1     680\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"test: \")\n",
    "print(test[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    16384\n",
       "1     8302\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# объединим train и dev, будем тренировать на обоих\n",
    "train = pd.concat([train, dev], axis=0, ignore_index=True)\n",
    "train[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Анализ разметки (2 балла)\n",
    "Функция ниже преобразует оффсеты в колонках в числа и подсчитывает всевозможные сочетания оффсетов в датафрейме. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getOffsetVariants(df_):\n",
    "    \n",
    "    df = df_.loc[df_['class'] == 1]\n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    cV = []\n",
    "    cR1 = []\n",
    "    cR2 = []\n",
    "    V = []\n",
    "    R1 = []\n",
    "    R2 = []\n",
    "    \n",
    "    for i in df.index:\n",
    "        cV.append(len(str(df[\"cV\"][i]).split(\" \"))) if df[\"cV\"][i] != 0 else cV.append(0)\n",
    "        cR1.append(len(str(df[\"cR1\"][i]).split(\" \"))) if df[\"cR1\"][i] != 0 else cR1.append(0)\n",
    "        cR2.append(len(str(df[\"cR2\"][i]).split(\" \"))) if df[\"cR2\"][i] != 0 else cR2.append(0)\n",
    "        V.append(len(str(df[\"V\"][i]).split(\" \"))) if df[\"V\"][i] != 0 else V.append(0)\n",
    "        R1.append(len(str(df[\"R1\"][i]).split(\" \"))) if df[\"R1\"][i] != 0 else R1.append(0)\n",
    "        R2.append(len(str(df[\"R2\"][i]).split(\" \"))) if df[\"R2\"][i] != 0 else R2.append(0)\n",
    "        \n",
    "    assert len(cV) == len(cR1) == len(cR2) == len(V) == len(R1) == len(R2)\n",
    "    if not len(cV) == len(cR1) == len(cR2) == len(V) == len(R1) == len(R2):\n",
    "        raise AssertionError()\n",
    "        \n",
    "    df_cols = ['cV','cR1', 'cR2', 'V', 'R1', 'R2']\n",
    "    \n",
    "    table = pd.DataFrame({\"cV\": cV,\n",
    "                          \"cR1\": cR1,\n",
    "                          \"cR2\": cR2,\n",
    "                          \"V\": V, \n",
    "                          \"R1\": R1,  \n",
    "                          \"R2\": R2},  columns=df_cols)\n",
    "    \n",
    "    return table.groupby(df_cols).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py:4034: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cV  cR1  cR2  V  R1  R2\n",
       "1   0    1    1  1   1        1\n",
       "    1    0    1  1   0      131\n",
       "         1    1  1   1     7621\n",
       "              2  2   2      461\n",
       "              3  3   3       63\n",
       "              4  4   4       21\n",
       "              5  5   5        2\n",
       "              7  7   7        2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getOffsetVariants(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cV  cR1  cR2  V  R1  R2\n",
       "1   1    0    1  1   0      16\n",
       "              2  2   0       1\n",
       "         1    1  1   1     616\n",
       "              2  2   2      42\n",
       "              3  3   3       3\n",
       "              4  4   4       1\n",
       "              5  5   5       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getOffsetVariants(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что чаще всего в предложениях встречается один оффсет для каждой метки, но есть достаточно много предложений с двумя и более оффсетами для некоторых меток (V, R1, R2). Там, где нет метки R2 (=0), скорее всего эллипсис находится в конце предложения. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Конвертер разметки-1 (1 балл)\n",
    "\n",
    "Я выбрала IOB-разметку. Эта функция далась мне тяжело (заняла много времени), во-первых, из-за иногда неправильно отмеченных оффсетов в датасете, и, во-вторых, из-за ситуаций, когда между двумя словами, разделенными тире или дефисом, не было пробела. Мой код ниже неплохо справляется с данной задачей, но, к сожалению, уже ближе к дедлайну я обнаружила, что число тегов не всегда соответствует числу слов в очищенном от пунктуации предложении. Пришлось пожертвовать примерно 200-ми предложениями, чтобы подготовить данные для RNN без ошибок. Функция ниже. \n",
    "(Обратно из разметки можно перевести, используя индекс предложения). \n",
    "\n",
    "P.S. Буду благодарна, если в комментариях к домашней работе будут советы по более оптимальному решению, учитывающему все нюансы.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent): # функция токенизирует предложение и очищает от пунктуации\n",
    "    sent = sent.replace(\",\", \", \")\n",
    "    words = [word.rstrip(punct) for word in sent.split() if word.rstrip(punct)]\n",
    "    return words\n",
    "\n",
    "def flatten(list_): # функция делает из списков со вложенными списками список с одним уровнем вложенности\n",
    "     for item in list_:\n",
    "        if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "             for x in flatten(item):\n",
    "                    yield x\n",
    "        else:        \n",
    "             yield item\n",
    "\n",
    "def get_offsets(df, sent_indx, column_name): # функция извлекает слайсы предложений по оффсетам\n",
    "    offsets = str(df[column_name][sent_indx]).split(\" \")\n",
    "    return [j.split(\":\") for j in offsets if offsets != [\"nan\"]]\n",
    "\n",
    "def makeIOBtags(df): # непосредстенно сама функция разметки\n",
    "     \n",
    "    tagged_sents = {} # данные положим в словарь\n",
    "    \n",
    "    for indx in df.index:  \n",
    "        if df[\"class\"][indx] == 1:\n",
    "            sent = df[\"text\"][indx]\n",
    "        \n",
    "            cV = get_offsets(df, indx, \"cV\") # извлекаем слайсы\n",
    "            cR1 = get_offsets(df, indx, \"cR1\")\n",
    "            cR2 = get_offsets(df, indx, \"cR2\")\n",
    "            R1 = get_offsets(df, indx, \"R1\")\n",
    "            R2 = get_offsets(df, indx, \"R2\")\n",
    "        \n",
    "            cV_words = [sent[int(offset[0]):int(offset[1])] for offset in cV] # берем слова по слайсам согласно меткам\n",
    "            cR1_words = [sent[int(offset[0]):int(offset[1])] for offset in cR1]\n",
    "            cR2_words = [sent[int(offset[0]):int(offset[1])] for offset in cR2]\n",
    "            R1_words = [sent[int(offset[0]):int(offset[1])] for offset in R1]\n",
    "            R2_words = [sent[int(offset[0]):int(offset[1])] for offset in R2]\n",
    "        \n",
    "            for i, chunk in enumerate(cV_words):\n",
    "                sent = sent.replace(chunk, \" cV\" + \"_\" + str(i)) # заменяем слайсы предложений метками и номером оффсета\n",
    "            for i, chunk in enumerate(cR1_words):\n",
    "                sent = sent.replace(chunk, \" cR1\" + \"_\" + str(i))\n",
    "            for i, chunk in enumerate(cR2_words):\n",
    "                sent = sent.replace(chunk, \" cR2\" + \"_\" + str(i))\n",
    "            for i, chunk in enumerate(R1_words):\n",
    "                sent = sent.replace(chunk, \" R1\" + \"_\" + str(i))\n",
    "            for i, chunk in enumerate(R2_words):\n",
    "                sent = sent.replace(chunk, \" R2\" + \"_\" + str(i))\n",
    "                        \n",
    "            t_sent = tokenize(sent) # токенизируем предложение\n",
    "            labels = (\"cV\", \"cR1\", \"cR2\", \"R1\", \"R2\")          \n",
    "            try:\n",
    "                for i, word in enumerate(t_sent): # для каждой метки добавляем IOB-теги\n",
    "                    if not word.startswith(labels):\n",
    "                        t_sent[i] = \"O\"\n",
    "                    elif word.startswith(\"cV\"):\n",
    "                        tokenized_cV_words = [tokenize(i) for i in cV_words]\n",
    "                        chunk_number = int(word.split(\"_\")[1][0]) # учитываем номер оффсета, чтобы не ошибиться с числом слов в нем\n",
    "                        chunk_length = len(tokenized_cV_words[chunk_number])\n",
    "                        if chunk_length == 1:\n",
    "                            t_sent[i] = \"B-cV\"\n",
    "                        elif chunk_length > 1:\n",
    "                            t_sent[i] = ([\"B-cV\"] + [\"I-cV\" for i in range(chunk_length-1)])\n",
    "                    elif word.startswith(\"cR1\"):\n",
    "                        tokenized_cR1_words = [tokenize(i) for i in cR1_words]\n",
    "                        chunk_number = int(word.split(\"_\")[1][0])\n",
    "                        chunk_length = len(tokenized_cR1_words[chunk_number])\n",
    "                        if chunk_length == 1:\n",
    "                            t_sent[i] = \"B-cR1\"\n",
    "                        elif chunk_length > 1:\n",
    "                            t_sent[i] = ([\"B-cR1\"] + [\"I-cR1\" for i in range(chunk_length-1)])\n",
    "                    elif word.startswith(\"cR2\"):\n",
    "                        tokenized_cR2_words = [tokenize(i) for i in cR2_words]\n",
    "                        chunk_number = int(word.split(\"_\")[1][0])\n",
    "                        chunk_length = len(tokenized_cR2_words[chunk_number])\n",
    "                        if chunk_length == 1:\n",
    "                            t_sent[i] = \"B-cR2\"\n",
    "                        elif chunk_length > 1:\n",
    "                            t_sent[i] = ([\"B-cR2\"] + [\"I-cR2\" for i in range(chunk_length-1)])\n",
    "                    elif word.startswith(\"R1\"):\n",
    "                        tokenized_R1_words = [tokenize(i) for i in R1_words]\n",
    "                        chunk_number = int(word.split(\"_\")[1][0])\n",
    "                        chunk_length = len(tokenized_R1_words[chunk_number])\n",
    "                        if chunk_length == 1:\n",
    "                            t_sent[i] = \"B-R1\"\n",
    "                        elif chunk_length > 1:\n",
    "                            t_sent[i] = ([\"B-R1\"] + [\"I-R1\" for i in range(chunk_length-1)])\n",
    "                    elif word.startswith(\"R2\"):\n",
    "                        tokenized_R2_words = [tokenize(i) for i in R2_words]\n",
    "                        chunk_number = int(word.split(\"_\")[1][0])\n",
    "                        chunk_length = len(tokenized_R2_words[chunk_number])\n",
    "                        if chunk_length == 1:\n",
    "                            t_sent[i] = \"B-R2\"\n",
    "                        elif chunk_length > 1:\n",
    "                            t_sent[i] = ([\"B-R2\"] + [\"I-R2\" for i in range(chunk_length-1)])\n",
    "            except ValueError and IndexError:\n",
    "                continue\n",
    "            \n",
    "            tags = list(flatten(t_sent)) # выравниваем список\n",
    "            tagged_sents[indx] = tags # добавляем в словарь и сохраняем номер предложения\n",
    "        \n",
    "        elif df[\"class\"][indx] == 0:  \n",
    "            sent = train[\"text\"][indx]\n",
    "            sent_length = len(tokenize(sent))\n",
    "            tags = [\"O\" for i in range(sent_length)]\n",
    "            tagged_sents[indx] = tags\n",
    "                        \n",
    "    return tagged_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_for_train = makeIOBtags(train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-cR1', 'I-cR1', 'B-cV', 'O', 'B-cR2', 'O', 'O', 'B-R1', 'B-R2', 'I-R2']\n",
      "Работа с двухбайтовыми наборами символов — просто кошмар для программиста, так как часть их состоит из одного байта, а часть — из двух.\n"
     ]
    }
   ],
   "source": [
    "print(tags_for_train[1], train.text[1], sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Функция будет использоваться ниже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Конвертер разметки-2 (1 балл)\n",
    "\n",
    "Простая и понятная функция. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsets2brackets(df):\n",
    "    sents = {}\n",
    "    for indx in df.index:  \n",
    "        if df[\"class\"][indx] == 1:\n",
    "            sent = df[\"text\"][indx]\n",
    "            \n",
    "            cV = get_offsets(df, indx, \"cV\")\n",
    "            cR1 = get_offsets(df, indx, \"cR1\")\n",
    "            cR2 = get_offsets(df, indx, \"cR2\")\n",
    "            V = get_offsets(df, indx, \"V\") # не забываем про сам эллипсис\n",
    "            R1 = get_offsets(df, indx, \"R1\")\n",
    "            R2 = get_offsets(df, indx, \"R2\")\n",
    "            \n",
    "            cV_words = [sent[int(offset[0]):int(offset[1])] for offset in cV]\n",
    "            cR1_words = [sent[int(offset[0]):int(offset[1])] for offset in cR1]\n",
    "            cR2_words = [sent[int(offset[0]):int(offset[1])] for offset in cR2]\n",
    "            R1_words = [sent[int(offset[0]):int(offset[1])] for offset in R1]\n",
    "            R2_words = [sent[int(offset[0]):int(offset[1])] for offset in R2]\n",
    "            \n",
    "            for offset in V:\n",
    "                sent = sent[:int(offset[0])] + \"V[] \" + sent[int(offset[0]):]\n",
    "             \n",
    "            for chunk in cV_words:\n",
    "                new_chunk = \"cV[ \" + chunk + \" cV]\"\n",
    "                sent = sent.replace(chunk, new_chunk)\n",
    "                \n",
    "            for chunk in cR1_words:\n",
    "                new_chunk = \"cR1[ \" + chunk + \" cR1]\"\n",
    "                sent = sent.replace(chunk, new_chunk)\n",
    "                \n",
    "            for chunk in cR2_words:\n",
    "                new_chunk = \"cR2[ \" + chunk + \" cR2]\"\n",
    "                sent = sent.replace(chunk, new_chunk)\n",
    "            \n",
    "            for chunk in R1_words:\n",
    "                new_chunk = \"R1[ \" + chunk + \" R1]\"\n",
    "                sent = sent.replace(chunk, new_chunk)\n",
    "            \n",
    "            for chunk in R2_words:\n",
    "                new_chunk = \"R2[ \" + chunk + \" R2]\"\n",
    "                sent = sent.replace(chunk, new_chunk)\n",
    "                            \n",
    "            sents[indx] = sent\n",
    "            \n",
    "    return sents\n",
    "            \n",
    "sents = offsets2brackets(train[156:157])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cR1[ Я cR1] и так этого года cV[ ждала cV] наверное cR2[ 9 cR2] лет а R1[ мама R1] V[] R2[ 11 R2].'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[156]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Бинарная классификация (6 баллов (+2))\n",
    "Постройте простую полносвязную сеть, берущую на вход все предложение и на выходе предсказывающую наличие или отсутствие гэппинга в предложении. Признаки: использование эмбеддингов +1 балл, использование pos-тэгов +1 балл. \n",
    "\n",
    "##### В данном задании я много экспериментировала, в итоге остановилась на конфигурации многослойного перцептрона на keras. Использовала и эмбеддинги, и POS-теги. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим предложения в виде векторов с эмбеддингами и частями речи для каждого слова. Для начала лемматизируем и соберем частеречные теги. \n",
    "\n",
    "### 5.1 лемматизируем данные и получим POS-теги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analyzeWithPunct(sents): # лемматизируем с сохранением пунктуации\n",
    "    all_lemmas = []\n",
    "    all_tags = []\n",
    "    for sent in sents:\n",
    "        analysis = m.analyze(sent)\n",
    "        lemmas = []\n",
    "        tags = []\n",
    "        for word in analysis:\n",
    "            try:\n",
    "                pos = word[\"analysis\"][0][\"gr\"].split(\",\")[0].split(\"=\")[0]\n",
    "                lemma = word[\"analysis\"][0][\"lex\"]\n",
    "                tags.append(pos)\n",
    "                lemmas.append(lemma)\n",
    "            except:\n",
    "                symbol = word[\"text\"]\n",
    "                if symbol not in \" \\n\": \n",
    "                    lemma = symbol.strip() # сохраняем пунктуацию (и другой шум) как отдельный токен\n",
    "                    if symbol.strip() == \",\":\n",
    "                        pos = \"COMMA\"\n",
    "                    elif symbol.strip() in \"—-\":\n",
    "                        pos = \"DASH\"\n",
    "                    else:\n",
    "                        pos = \"PUNCT\"\n",
    "                    tags.append(pos) \n",
    "                    lemmas.append(lemma)\n",
    "        all_lemmas.append(lemmas)\n",
    "        all_tags.append(tags)\n",
    "    return all_lemmas, all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lemmas, train_pos = analyzeWithPunct([i for i in train.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['работа', 'с', 'двухбайтовый', 'набор', 'символ', '—', 'просто', 'кошмар', 'для', 'программист', ',', 'так', 'как', 'часть', 'они', 'состоять', 'из', 'один', 'байт', ',', 'а', 'часть', '—', 'из', 'два', '.'] ['S', 'PR', 'A', 'S', 'S', 'DASH', 'PART', 'S', 'PR', 'S', 'COMMA', 'ADVPRO', 'ADVPRO', 'S', 'SPRO', 'V', 'PR', 'ANUM', 'S', 'COMMA', 'CONJ', 'S', 'DASH', 'PR', 'NUM', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "print(train_lemmas[1], train_pos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lemmas, test_pos = analyzeWithPunct([i for i in test.text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Чтобы создать входной вектор определенной длины, узнаем максимальное число слов в предложениях train и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.8590401983354\n"
     ]
    }
   ],
   "source": [
    "lemmas = train_lemmas + test_lemmas\n",
    "len_ = 0\n",
    "counter = 0\n",
    "for i in lemmas:\n",
    "    counter += 1\n",
    "    len_ += len(i)\n",
    "avr_len = len_/counter \n",
    "print(avr_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, средняя длина предложений - 20. Мы будем считать, что в среднем предложении 25 слов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Нормализуем POS-теги и преобразуем в one-hot вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим словарь, который нормализует число POS-тегов до 25 на предложение \n",
    "# если в предложени более 25 слов, оно обрезается сначала, так как эллипсис чаще встречается в конце предложения\n",
    "# также данная функция приводит теги Mystem к более общему виду: например, прилагательное-числительное у нас будет просто прилагательным\n",
    "def tag2vec(tags_list):\n",
    "    dictionary = {\"V\":\"VERB\", \"S\":\"NOUN\", \"SPRO\":\"NOUN\", \"A\":\"ADJ\", \"ANUM\":\"ADJ\", \"APRO\":\"ADJ\", \n",
    "                  \"ADV\":\"ADV\", \"ADVPRO\":\"ADV\", \"CONJ\":\"CONJ\", \"PR\":\"PR\", \"PART\":\"PART\", \"INTJ\":\"INTJ\", \n",
    "                  \"NUM\":\"NUM\", \"COM\":\"COM\", \"COMMA\":\"COMMA\", \"DASH\":\"DASH\", \"PUNCT\":\"PUNCT\"}\n",
    "    tags_vec = [[dictionary[tag] for tag in sent] for sent in tags_list]\n",
    "    for i, vec in enumerate(tags_vec):\n",
    "        if len(vec) > 25:\n",
    "            tags_vec[i] = vec[len(vec) - 25 : ]\n",
    "        elif len(vec) < 25:\n",
    "            tags_vec[i] = vec + [\"0\" for i in range(25 - len(vec))]\n",
    "    return tags_vec\n",
    "\n",
    "train_tags_vec = tag2vec(train_pos)\n",
    "test_tags_vec = tag2vec(test_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADJ', 'NOUN', 'VERB', 'VERB', 'PUNCT', 'NOUN', 'NOUN', 'PUNCT', 'NOUN', 'NOUN', 'COMMA', 'CONJ', 'ADJ', 'PUNCT', 'PUNCT', 'NOUN', 'ADJ', 'NOUN', 'PUNCT', '0', '0', '0', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "print(train_tags_vec[8]) # получается вот так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      VERB\n",
       "1      NOUN\n",
       "2       ADJ\n",
       "3       ADV\n",
       "4      CONJ\n",
       "5        PR\n",
       "6      PART\n",
       "7      INTJ\n",
       "8       NUM\n",
       "9       COM\n",
       "10    COMMA\n",
       "11     DASH\n",
       "12    PUNCT\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# переведем  pos-теги в one-hot вектора\n",
    "pos_list = [\"VERB\", \"NOUN\", \"ADJ\", \"ADV\", \"CONJ\", \"PR\", \"PART\", \"INTJ\", \"NUM\", \"COM\", \"COMMA\", \"DASH\", \"PUNCT\"]\n",
    "pos_list = pd.DataFrame(pos_list)\n",
    "pos_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADV</th>\n",
       "      <th>COM</th>\n",
       "      <th>COMMA</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>DASH</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PART</th>\n",
       "      <th>PR</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>VERB</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ADJ  ADV  COM  COMMA  CONJ  DASH  INTJ  NOUN  NUM  PART  PR  PUNCT  VERB  \\\n",
       "0     0    0    0      0     0     0     0     0    0     0   0      0     1   \n",
       "1     0    0    0      0     0     0     0     1    0     0   0      0     0   \n",
       "2     1    0    0      0     0     0     0     0    0     0   0      0     0   \n",
       "3     0    1    0      0     0     0     0     0    0     0   0      0     0   \n",
       "4     0    0    0      0     1     0     0     0    0     0   0      0     0   \n",
       "5     0    0    0      0     0     0     0     0    0     0   1      0     0   \n",
       "6     0    0    0      0     0     0     0     0    0     1   0      0     0   \n",
       "7     0    0    0      0     0     0     1     0    0     0   0      0     0   \n",
       "8     0    0    0      0     0     0     0     0    1     0   0      0     0   \n",
       "9     0    0    1      0     0     0     0     0    0     0   0      0     0   \n",
       "10    0    0    0      1     0     0     0     0    0     0   0      0     0   \n",
       "11    0    0    0      0     0     1     0     0    0     0   0      0     0   \n",
       "12    0    0    0      0     0     0     0     0    0     0   0      1     0   \n",
       "\n",
       "    0  \n",
       "0   0  \n",
       "1   0  \n",
       "2   0  \n",
       "3   0  \n",
       "4   0  \n",
       "5   0  \n",
       "6   0  \n",
       "7   0  \n",
       "8   0  \n",
       "9   0  \n",
       "10  0  \n",
       "11  0  \n",
       "12  0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_pos = pd.get_dummies(pos_list[0])\n",
    "one_hot_pos[\"0\"] = [0 for i in range(13)]\n",
    "one_hot_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tags_vec = [[one_hot_pos[i].tolist() for i in tags] for tags in test_tags_vec]\n",
    "train_tags_vec = [[one_hot_pos[i].tolist() for i in tags] for tags in train_tags_vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Эмбеддинги и окончательные вектора признаков\n",
    "Я беру эмбеддинги из модели fastText, предобученной на собранном мной корпусе статей Новой газеты за 2015-2018 годы из рублик \"Культура\" и \"Общество\". Обучение велось на леммах (4325550 токенов)), без предлогов, местоимений и других стоп-слов. Размерность эмбеддингов в моей модели - 50.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "model2 = FastText.load_fasttext_format('fasttext/model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция объединяет признаки слов для предложения\n",
    "\n",
    "def get_vector(sent, tags, model):\n",
    "    matrix = np.zeros((25,50+13)) # размерность вектора предложения = 1575 (50 - эмбеддинг, 13 - POS-tag)\n",
    "    if len(sent) > 25:\n",
    "        sent = sent[len(sent) - 25 : ]\n",
    "    for i, lemma in enumerate(sent):\n",
    "        if lemma in model2.wv:\n",
    "            vec = np.append(model2.wv[lemma], tags[i]) # эмбеддинг и POS-тег для каждого слова\n",
    "            matrix[i] = vec\n",
    "        else:\n",
    "            pass\n",
    "    vector = np.concatenate(matrix)\n",
    "    return vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = [] # получаем вектора для train\n",
    "for i in range(len(train_lemmas)):\n",
    "    sent_vec = get_vector(train_lemmas[i], train_tags_vec[i], model2)\n",
    "    train_vec.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = [] # получаем вектора для test\n",
    "for i in range(len(test_lemmas)):\n",
    "    sent_vec = get_vector(test_lemmas[i], test_tags_vec[i], model2)\n",
    "    test_vec.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [train_set[\"class\"][i] for i in train_set.index] # и классы\n",
    "test_labels = [test[\"class\"][i] for i in test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные для модели почти готовы!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Модель на Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "import keras_metrics as km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.stack(train_vec, axis=0) # преобразуем данные так, чтобы Keras прочел\n",
    "x_test = np.stack(test_vec, axis=0)\n",
    "y_train = np.asarray(train_labels)\n",
    "y_test = np.asarray(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_82\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_195 (Dense)            (None, 300)               472800    \n",
      "_________________________________________________________________\n",
      "activation_195 (Activation)  (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_114 (Dropout)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_196 (Dense)            (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "activation_196 (Activation)  (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_115 (Dropout)        (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_197 (Dense)            (None, 1)                 201       \n",
      "_________________________________________________________________\n",
      "activation_197 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 533,201\n",
      "Trainable params: 533,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# проектируем модель\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(300, input_dim=1575), # на вход идет вектор размерностью 1575 (т.е. 50+13 (эмбеддинг + pos-тег на слово) * 25)\n",
    "    Activation('relu'), # 3 скрытых слоя\n",
    "    Dropout(0.5),\n",
    "    Dense(200),\n",
    "    Activation(\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(1),\n",
    "    Activation('sigmoid') \n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20543 samples, validate on 2045 samples\n",
      "Epoch 1/5\n",
      "20543/20543 [==============================] - 8s 393us/step - loss: 0.0775 - precision: 0.9616 - recall: 0.9785 - val_loss: 0.8116 - val_precision: 0.6546 - val_recall: 0.7721\n",
      "Epoch 2/5\n",
      "20543/20543 [==============================] - 8s 403us/step - loss: 0.0795 - precision: 0.9660 - recall: 0.9763 - val_loss: 0.8654 - val_precision: 0.6335 - val_recall: 0.8059\n",
      "Epoch 3/5\n",
      "20543/20543 [==============================] - 8s 396us/step - loss: 0.0740 - precision: 0.9632 - recall: 0.9760 - val_loss: 0.8809 - val_precision: 0.6453 - val_recall: 0.7706\n",
      "Epoch 4/5\n",
      "20543/20543 [==============================] - 8s 383us/step - loss: 0.0714 - precision: 0.9693 - recall: 0.9767 - val_loss: 0.8746 - val_precision: 0.6514 - val_recall: 0.7750\n",
      "Epoch 5/5\n",
      "20543/20543 [==============================] - 8s 371us/step - loss: 0.0707 - precision: 0.9682 - recall: 0.9764 - val_loss: 0.9692 - val_precision: 0.6349 - val_recall: 0.8029\n"
     ]
    }
   ],
   "source": [
    "# компилируем модель. В качестве оптимизатора используем Adam \n",
    "# и в качестве функции потерь - кроссэнтропию для бинарной классификации\n",
    "\n",
    "model.compile( \n",
    " optimizer = \"adam\",\n",
    " loss = \"binary_crossentropy\", \n",
    " metrics = [km.binary_precision(), km.binary_recall()]\n",
    ")\n",
    "\n",
    "results = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs = 5, # вроде бы 5 эпох достаточно\n",
    "    batch_size = 50,\n",
    "    validation_data = (x_test, y_test), # валидируем на тесте для удобства\n",
    "    class_weight = {0:1., 1:2.} # балансируем классы\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7090909089988193"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test)\n",
    "score[1] * score[2] * 2 / (score[1] + score[2])\n",
    "# получаем такую F-меру "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Оценка результатов бинарной классификации (1 балл)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(x_test) # предсказываем\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame([test.text[i] for i in test.index], columns = [\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df[\"class\"] = predictions # размечаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Изобретение относится к судостроению и касаетс...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Эти состояния называют фазами воды, а превраще...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>И должен ни единой долькой  Не отступаться от ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Он потребовал обеспечить полное осуществление ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>По мнению местного пастора Элла Эбанкса, запре...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  class\n",
       "0  Изобретение относится к судостроению и касаетс...      0\n",
       "1  Эти состояния называют фазами воды, а превраще...      1\n",
       "2  И должен ни единой долькой  Не отступаться от ...      0\n",
       "3  Он потребовал обеспечить полное осуществление ...      0\n",
       "4  По мнению местного пастора Элла Эбанкса, запре...      0"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(\"output.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скрипт agrr_metrics.py выдал следующую оценку:\n",
    "````\n",
    "Binary classification quality (f1-score): 0.7131258457374832\n",
    "Other metrics: \n",
    " Precision: 0.6604010025062657\n",
    " Recall: 0.775\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-мера составила 0.71. Надеюсь, это неплохой результат, учитывая то, что это обычная сеть, и то, что здесь не использовался синтаксис в качестве признаков.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Классификация последовательности (7 баллов) \n",
    "\n",
    "Для начала напишем функцию, которая преобразует слова в предложениях в индексы из словаря, а IOB-теги - в one-hot вектора. Впоследствии нейронная сеть сделает из индексов слов эмбеддинги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 60 # пока возьмем максимальную длину предложений, равную 60\n",
    "\n",
    "\n",
    "def prepareXY(df): # функция принимает на вход весь датафрейм\n",
    "    IOB_tags = makeIOBtags(df) # сделаем разметку по IOB-тегам для каждого предложения\n",
    "    \n",
    "    for indx in df.index:\n",
    "        if indx not in IOB_tags.keys():\n",
    "            df = df[df.index != indx] # поскольку эта разметка не идеальна (в том числе из-за самих данных)\n",
    "                                      # удаляем из датафрейма предложения, которые не разметились\n",
    "                                      # (в тесте все размечается хорошо, в трейне - нет)\n",
    "    \n",
    "    to_delete = []\n",
    "    counter = 0\n",
    "    for i in df:\n",
    "        try:\n",
    "            if len(df[i]) != len(tokenize(df.text[i])):  # или разметились неправильно (число тегов не равно числу токенов в предложении)\n",
    "                counter += 1\n",
    "                df = df[df.index != i]\n",
    "                to_delete.append(i)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    for i in to_delete:\n",
    "        del IOB_tags[i]\n",
    "    \n",
    "    df[\"IOB\"] = [v for v in IOB_tags.values()] \n",
    "    print(df[\"class\"].value_counts()) # после всех преобразований в датафрейме останется столько предложений\n",
    "    \n",
    "    sents = [tokenize(df.text[i]) for i in df.index] \n",
    "    for i, sent in enumerate(sents): # обрежем начало предложений у тех, в которых число слов > 60. \n",
    "        if len(sent) > max_len:\n",
    "            sents[i] = sent[len(sent) - max_len : ]\n",
    "    tags = [df.IOB[i] for i in df.index] \n",
    "    for i, sent_tags in enumerate(tags):  # и обрежем теги\n",
    "        if len(sent_tags) > 60:\n",
    "            tags[i] = sent_tags[len(sent_tags) - max_len : ]\n",
    "    \n",
    "    sent_tuples = []\n",
    "    for i in range(len(sents)):\n",
    "        sent_tuples.append([(w, t) for w, t in zip(sents[i], tags[i])])\n",
    "    \n",
    "    words = list(set(list(flatten(sents)))) # создадим словарь из токенов\n",
    "    words.append(\"ENDPAD\") # добавим тег, который будет заполнять пустое место в предложениях, где число слов < 60\n",
    "    print(str(len(words)) + \" words\")\n",
    "    tags = list(set(list(flatten(tags))))\n",
    "    \n",
    "    word2idx = {w: i for i, w in enumerate(words)}  \n",
    "    tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "        \n",
    "    X = [[word2idx[w[0]] for w in s] for s in sent_tuples] # закодируем слова в числа в предложениях\n",
    "    X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=len(words) - 1) # сделаем паддинг\n",
    "    Y = [[tag2idx[w[1]] for w in s] for s in sent_tuples] # закодируем IOB-теги в числа в предложениях\n",
    "    Y = pad_sequences(maxlen=max_len, sequences=Y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "    Y = [to_categorical(i, num_classes=len(tag2idx)) for i in Y] # преобразуем их в one-hot вектора\n",
    "\n",
    "    return X, Y, words, tags, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    13624\n",
      "1     6920\n",
      "Name: class, dtype: int64\n",
      "90251 words\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y, train_words, train_tags, train = prepareXY(train) # готовим данные для трейна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1365\n",
      "1     680\n",
      "Name: class, dtype: int64\n",
      "17570 words\n"
     ]
    }
   ],
   "source": [
    "test_X, test_Y, test_words, test_tags, test = prepareXY(test) # и для теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14813, 63305, 38148, 18497, 75263, 79443, 47497, 61148,   735,\n",
       "       70585, 40939, 42421, 53807, 41264, 54837,  8735,  2495, 78535,\n",
       "       42421, 54837, 42215, 90250, 90250, 90250, 90250, 90250, 90250,\n",
       "       90250, 90250, 90250, 90250, 90250, 90250, 90250, 90250, 90250,\n",
       "       90250, 90250, 90250, 90250, 90250, 90250, 90250, 90250, 90250,\n",
       "       90250, 90250, 90250, 90250, 90250, 90250, 90250, 90250, 90250,\n",
       "       90250, 90250, 90250, 90250, 90250, 90250], dtype=int32)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем модель на keras, двустороннюю LSTM, которая будет преобразовывать слова в эмбеддинги и запоминать их последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape=(max_len,))\n",
    "model = Embedding(input_dim=len(words), output_dim=50, input_length=max_len)(input) \n",
    "model = Dropout(0.1)(model)\n",
    "model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "out = TimeDistributed(Dense(len(tags), activation=\"softmax\"))(model)  # softmax output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18301 samples, validate on 2034 samples\n",
      "Epoch 1/10\n",
      "18301/18301 [==============================] - 141s 8ms/step - loss: 0.0451 - acc: 0.9856 - val_loss: 0.2534 - val_acc: 0.9245\n",
      "Epoch 2/10\n",
      "18301/18301 [==============================] - 113s 6ms/step - loss: 0.0397 - acc: 0.9873 - val_loss: 0.2901 - val_acc: 0.9133\n",
      "Epoch 3/10\n",
      "18301/18301 [==============================] - 112s 6ms/step - loss: 0.0346 - acc: 0.9890 - val_loss: 0.2930 - val_acc: 0.9150\n",
      "Epoch 4/10\n",
      "18301/18301 [==============================] - 114s 6ms/step - loss: 0.0309 - acc: 0.9903 - val_loss: 0.2763 - val_acc: 0.9219\n",
      "Epoch 5/10\n",
      "18301/18301 [==============================] - 113s 6ms/step - loss: 0.0276 - acc: 0.9913 - val_loss: 0.3205 - val_acc: 0.9115\n",
      "Epoch 6/10\n",
      "18301/18301 [==============================] - 114s 6ms/step - loss: 0.0246 - acc: 0.9922 - val_loss: 0.3333 - val_acc: 0.9089\n",
      "Epoch 7/10\n",
      "18301/18301 [==============================] - 111s 6ms/step - loss: 0.0223 - acc: 0.9929 - val_loss: 0.3428 - val_acc: 0.9125\n",
      "Epoch 8/10\n",
      "18301/18301 [==============================] - 114s 6ms/step - loss: 0.0198 - acc: 0.9938 - val_loss: 0.3813 - val_acc: 0.9011\n",
      "Epoch 9/10\n",
      "18301/18301 [==============================] - 114s 6ms/step - loss: 0.0182 - acc: 0.9942 - val_loss: 0.3618 - val_acc: 0.9083\n",
      "Epoch 10/10\n",
      "18301/18301 [==============================] - 111s 6ms/step - loss: 0.0164 - acc: 0.9948 - val_loss: 0.3728 - val_acc: 0.8992\n"
     ]
    }
   ],
   "source": [
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# в качестве оптимизатора применим RMSProp — root mean square propagation\n",
    "history = model.fit(train_X, np.array(train_Y), batch_size=32, epochs=10, validation_split=0.1, verbose=1)\n",
    "# будем обучать на 10 эпохах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAFpCAYAAABwPvjcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xt81PWd7/HXXHK/kZnJ/cIlgo0goI2KWJGQuN217mM51tOz2nLawq5nq+LZtqigYFWkZS2W1ktx2wPsli5b9nRre2rbVQNGrWkJSLFiEESBXCHJhGRymyST3+/8MSFhuJgACRPm934+HnkMk/n9ku/vQ/LOd76/7+/7s5mmaSIiIpZgD3cDRETk0lHoi4hYiEJfRMRCFPoiIhai0BcRsRCFvoiIhSj0RUQsRKEvImIhCn0REQtR6IuIWIhCX0TEQpzhbsDZ1NfXX/C+Ho+H5ubmUWzN5Uu1CKV6hFI9hkRCLbKzs0e0nXr6IiIWotAXEbEQhb6IiIUo9EVELEShLyJiIQp9ERELUeiLiFiIQl9ExEIU+iIiFqLQFxGxEIW+iIiFjMu1d0REIpkZCECPH/zd0NM98OiHqChsV1w1pt9boS8i8glM04TeXujpAr9/KKAHAts87Tn+bvD7MXuCj0OfO2W7QN/Zv9nkaTgeWTemx6PQF5GIYJomBALQ6w+Gba8/GLIDH2ZPTzCAe3rO2KYNk35f2ynhfGpg+8E0RtYIhwNi4iA2DmJig4+xcZA0AVtsHMTGDrw+8NrAtraT2yYmj22RUOiLSBiYhgHdXdDpGwrfgcdgOJ8S2L0jCe6BzxsjDGcAmw2iYyE2lr64eIiKDoZwUgo2T8ZQYJ8M8YHAtp0W2IPhHhOHLSpq7Io2ShT6InLRzJ4e6PBBRxu0+zA7fMHn7cHHoedtwcfO9pEHdFQ0xMQEAzrmlI8J7mAPOTomGLrRMaGvR8cEe9eDn48Lfp2Tr0dFY7PZgMhYT3+kFPoiEsLs7w/2wNvbgyHe4cMcCO+TQT4Y4gOv09t79i9ms0NiUnDYIjEJsnKxJSYPPE+GhKRgz3mgx31GsMfEYLM7Lm0BIpxCXyRCmYYRHJ/u6gh+dHZAVydmZzt0ddBuBDAaj2F2tJ/SK2+Drs5zf9G4+KHATknFljMRkoZC3HbKv0lKgbgEbHbNDB9PFPoi49jZg7sDc+Ax+LnO4OdOeZ2uzuDHJ5yA7HJGnRLQydgmFoQGdmIytsSkgVBPgcQkbM7xP2Ytn0yhL3IJmD09wSGTDt+oBjcOB8QnDnwkBMM7I3vocwkJEJ+ILSEp5DnxiXhycvF6vZeuCDIuKPRFzoNpmsFZIieHRDrag8MlA/8+eZJyaMy7PRj25xrzhhEEd/DDdlqQE58IMbGDJyPP14XuJ5c3hb5YlmmawWmDJ2eTdPiGxrcHHs1OX0jA0+kLzgU/G5stGMQnT1q60rDlTRl6npgc7HEnJI1acIucL4W+RBQz0Ae+Vmg7AW0nMNtOQFsLtLXS2tNN/4nm4AnLzvbgR3//2b/QyVknCQMzT9IysU2eFnyeNDDr5JQwJyEZEhI000TGPYW+jHuDQyqtA0HuGwjykOcDn+toP/sXSUohMMEVnH2SlRvscYf0wE8J8MRkiIvXrBOJSAp9CRvTMILDJgOBbba1DvTKT++ln4DenjO/gNMJyamQkhrsiV9RCCkuSJmALcUV/HxyKiRPwOZ0WuoCHJFzUejLqDNNMzj7xNsEJ5oxTzSfFuQDYe5rPftVmXHxg4FtmzQ1GOQTBp6npA4GOwlJGgsXOU8KfTlvZl8vnGgGbxNmSzOcaIKWZkxvE7QEg54ef+hONltw7ndKsGduy50YDO/kVGwTBnrrJ5/HxITnwEQsQKEvIUzDCPbATzRDS9NgkJsDIU9LU3D9lNMlTwBXGmTnY5txbXDmiisNXB5IdQdXGXToJKdIuCn0Lcb0d4F3INBPNA39+2QPvaUZ+k+bkhgTGwx0lwdb/pRgkA+GehqkurFFRYfngETkvCj0I4wZ6IPGBmiopbOjFaP2aOiwy+nrqtjtMMEdDPTJ0+DTNw0Eumcg6NMgPkFj5yIRQqF/mTL9XdBQh9lQA8dqMBtqoaEWmhoGT452QHBeucsDngxs06YPBvlgqKe4NOwiYiEK/XHMNM3g+HlD7UC4Dzw21AZ77Sc5HJCWBdl52D49NzgPPSsP91VX09LZHb4DEJFxR6E/DpiGAd7GgVCvDQ33zlMuNoqJhczcYI89Kw9bVi5k5UFaFjbnmf+V9rgEUOiLyCkU+peQGeiD4w0DwzE1Qz3443WhC3IlJgd76yd77Zl5wXBPdesqURG5KAr9MTCS8XYgOKaelYvtyqtDwt2WNPY3RxYRa1LoXyDTNKG1JTgUcywY6icfaT1ljXKHA9KzISc/ZLydjJzg/TtFRC4hhf4wzEAfNB075WRqXTDcj9UG72h0UmxccLz9UzMhMycY7Fl5wTVhzjLeLiISDkqjAWZXx0BvvS6k937GkMwEd7C3fuOCgSGZXMjMhQkuzWUXkXHPUqFvGgac8A6F+imzZWg7MbShwwnpWaFDMpm5wR58bHz4DkBE5CJFZOibfX3QWH/K1MeBIZnjdaELgcUnBE+czrg2ODSTOTAF0pOhC5ZEJCJFTOibvhMY//o8zY31GMcbQm8m7U4P9tKnTT8l3HOCi4BpSEZELCRiQp+4BGhpxlnwKYyim4dOpmbkaKleEZEBERP6tqhoHN/6ARN0dyQRkXPS5Z0iIhai0BcRsRCFvoiIhSj0RUQsRKEvImIhCn0REQtR6IuIWIhCX0TEQhT6IiIWotAXEbEQhb6IiIWMaO2dvXv3snnzZgzDoKSkhIULF4a83tTUxIYNG/D5fCQmJrJ06VLcbjcAP/3pT9mzZw+maXL11Vfz1a9+VStbioiEybA9fcMw2LhxI4888gjr16/n7bffpra2NmSbLVu2MG/ePNatW8edd97J1q1bAThw4AAHDhxg3bp1PPPMM3z00UdUVVWNzZGIiMiwhg39Q4cOkZmZSUZGBk6nk7lz57Jr166QbWpra5kxYwYA06dPZ/fu3QDYbDZ6e3sJBAL09fXR399PSkrKGByGiIiMxLCh39LSMjhUA+B2u2lpaQnZZuLEiVRWVgJQWVlJd3c37e3tTJs2jenTp3PPPfdwzz33MGvWLHJzc0f5EEREZKRGZT39RYsWsWnTJsrLyyksLMTlcmG32zl27Bh1dXW8+OKLAKxevZr9+/dTWFgYsn9ZWRllZWUArF27Fo/Hc8FtcTqdF7V/JFEtQqkeoVSPIVaqxbCh73K58Hq9g8+9Xi8ul+uMbZYtWwaA3+9n586dJCQksH37dqZOnUpsbCwA11xzDQcPHjwj9EtLSyktLR18fjE3QfHoJiqDVItQqkco1WNIJNQiOzt7RNsNO7xTUFBAQ0MDjY2NBAIBKioqKCoqCtnG5/NhGMF70r700ksUFxcDwULu37+f/v5+AoEAVVVV5OTknO+xiIjIKBm2p+9wOFi8eDFr1qzBMAyKi4vJy8tj27ZtFBQUUFRURFVVFVu3bsVms1FYWMiSJUsAmDNnDvv27Rt8FzB79uwz/mCIiMilYzNN0wx3I05XX19/wftGwtu00aJahFI9QqkeQyKhFqM2vCMiIpFDoS8iYiEKfRERC1Hoi4hYiEJfRMRCFPoiIhai0BcRsRCFvoiIhSj0RUQsRKEvImIhCn0REQtR6IuIWIhCX0TEQhT6IiIWotAXEbEQhb6IiIUo9EVELEShLyJiIQp9ERELUeiLiFiIQl9ExEIU+iIiFqLQFxGxEIW+iIiFKPRFRCxEoS8iYiEKfRERC1Hoi4hYiEJfRMRCFPoiIhai0BcRsRCFvoiIhSj0RUQsRKEvImIhCn0REQtR6IuIWIhCX0TEQhT6IiIWotAXEbEQhb6IiIUo9EVELEShLyJiIQp9ERELUeiLiFiIQl9ExEIU+iIiFqLQFxGxEIW+iIiFKPRFRCxEoS8iYiEKfRERC1Hoi4hYiEJfRMRCnCPZaO/evWzevBnDMCgpKWHhwoUhrzc1NbFhwwZ8Ph+JiYksXboUt9sNQHNzMy+++CJerxeAFStWkJ6ePsqHISIiIzFs6BuGwcaNG1m5ciVut5sVK1ZQVFREbm7u4DZbtmxh3rx5zJ8/n3379rF161aWLl0KwPPPP88dd9zBzJkz8fv92Gy2sTsaERH5RMMO7xw6dIjMzEwyMjJwOp3MnTuXXbt2hWxTW1vLjBkzAJg+fTq7d+8e/Hx/fz8zZ84EIDY2lpiYmNE+BhERGaFhQ7+lpWVwqAbA7XbT0tISss3EiROprKwEoLKyku7ubtrb26mvrychIYF169bx0EMPsWXLFgzDGOVDEBGRkRrRmP5wFi1axKZNmygvL6ewsBCXy4XdbscwDPbv38/TTz+Nx+Nh/fr1lJeXs2DBgpD9y8rKKCsrA2Dt2rV4PJ4LbovT6byo/SOJahFK9QilegyxUi2GDX2XyzV4EhbA6/XicrnO2GbZsmUA+P1+du7cSUJCAi6Xi0mTJpGRkQHA9ddfz8GDB88I/dLSUkpLSwefNzc3X/ABeTyei9o/kqgWoVSPUKrHkEioRXZ29oi2G3Z4p6CggIaGBhobGwkEAlRUVFBUVBSyjc/nGxy2eemllyguLgbgiiuuoKurC5/PB8C+fftCTgCLiMilNWxP3+FwsHjxYtasWYNhGBQXF5OXl8e2bdsoKCigqKiIqqoqtm7dis1mo7CwkCVLlgBgt9tZtGgRTz75JKZpMmXKlJAevYiIXFo20zTNcDfidPX19Re8byS8TRstqkUo1SOU6jEkEmoxasM7IiISORT6IiIWotAXEbEQhb6IiIUo9EVELEShLyJiIQp9ERELUeiLiFiIQl9ExEIU+iIiFqLQFxGxEIW+iIiFKPRFRCxEoS8iYiEKfRERC1Hoi4hYiEJfRMRCFPoiIhai0BcRsRCFvoiIhSj0RUQsRKEvImIhCn0REQtR6IuIWIhCX0TEQhT6IiIWotAXEbEQhb6IiIUo9EVELEShLyJiIQp9ERELUeiLiFiIQl9ExEIU+iIiFqLQFxGxEIW+iIiFKPRFRCxEoS8iYiEKfRERC1Hoi4hYiEJfRMRCFPoiIhai0BcRsRCFvoiIhSj0RUQsRKEvImIhCn0REQtR6IuIWIhCX0TEQhT6IiIWotAXEbEQhb6IiIUo9EVELEShLyJiIc6RbLR37142b96MYRiUlJSwcOHCkNebmprYsGEDPp+PxMREli5ditvtHny9q6uLb3zjG1x33XUsWbJkdI9ARERGbNievmEYbNy4kUceeYT169fz9ttvU1tbG7LNli1bmDdvHuvWrePOO+9k69atIa9v27aNwsLC0W25iIict2FD/9ChQ2RmZpKRkYHT6WTu3Lns2rUrZJva2lpmzJgBwPTp09m9e/fgax9//DFtbW3MmjVrlJsuIiLna9jhnZaWlpChGrfbzYcffhiyzcSJE6msrOS2226jsrKS7u5u2tvbSUhI4Cc/+QlLly7lvffeO+f3KCsro6ysDIC1a9fi8Xgu9HhwOp0XtX8kUS1CqR6hVI8hVqrFiMb0h7No0SI2bdpEeXk5hYWFuFwu7HY7r776Ktdcc03IH42zKS0tpbS0dPB5c3PzBbfF4/Fc1P6RRLUIpXqEUj2GREItsrOzR7TdsKHvcrnwer2Dz71eLy6X64xtli1bBoDf72fnzp0kJCRw8OBB9u/fz6uvvorf7ycQCBAbG8sXv/jF8zkWEREZJcOGfkFBAQ0NDTQ2NuJyuaioqOCBBx4I2ebkrB273c5LL71EcXExQMh25eXlfPTRRwp8EZEwGjb0HQ4HixcvZs2aNRiGQXFxMXl5eWzbto2CggKKioqoqqpi69at2Gw2CgsLNS1TRGScspmmaYa7Eaerr6+/4H0jYWxutKgWoVSPUKrHkEioxUjH9HVFroiIhSj0RUQsRKEvImIhCn0REQtR6IuIWIhCX0TEQhT6IiIWotAXEbEQhb6IiIUo9EVELEShLyJiIQp9ERELUeiLiFiIQl9ExEIU+iIiFqLQFxGxEIW+iIiFKPRFRCxEoS8iYiEKfRERC1Hoi4hYiEJfRMRCFPoiIhai0BcRsRCFvoiIhSj0RUQsRKEvImIhCn0REQtR6IuIWIhCX0TEQhT6IiIWotAXEbEQhb6IiIUo9EVELEShLyJiIQp9ERELUeiLiFiIQl9ExEIU+iIiFqLQFxGxEIW+iIiFRFTo/6mhk75+I9zNEBEZtyIm9GvbenhiRw2P/e4AAcMMd3NERMaliAn93JQYlnw6nTc/8rK+op5+Bb+IyBmc4W7AaPrrT7mIjY/n+beO4LQ38MCcLBx2W7ibJSIybkRU6APcdW0urb4OfvpuM067jftuyMRuU/CLiEAEhj7Af5/hIWCY/Ow9L067jX+4LgObgl9EJDJDH+Bvr/bQ12/yn1UtOO02/u7T6Qp+EbG8iA19m83GotlpBAyTX31wAqfdxleuSVPwi4ilRWzoQzD4v3ptOgHD5Jf7gz3+L83yKPgvob5+g95+k4RoR7ibIiJEeOhDMPj/riiDgAE/f99LlMPG317tCXezLGFPfQcbKo/THTB4YkEeBa7YcDdJxPIiZp7+J7HbbPzD9RksmJLCv/+5mZ+/7w13kyKazx9g/dv1PPF6LVEOG3FOG6vKqvmgqTvcTROxvIjv6Z9kt9m4/4ZM+g2TLXubiLLb+JtCV7ibFVFM0+SNIz42vtNIV18/X5jh5r/PcNPm7+ex7dV8a0c1j96Sy8zMhHA3VcSyRhT6e/fuZfPmzRiGQUlJCQsXLgx5vampiQ0bNuDz+UhMTGTp0qW43W6OHDnCj3/8Y7q7u7Hb7dxxxx3MnTt3TA5kJBx2G//7xiwChsmmPY047TY+d2Vq2NoTSY539LKh8jh/auhkmjuW++fkM3FCDABpCXa+fetEHttezeryWlbMy+Ha7MQwt1jEmoYNfcMw2LhxIytXrsTtdrNixQqKiorIzc0d3GbLli3MmzeP+fPns2/fPrZu3crSpUuJjo7m/vvvJysri5aWFpYvX86sWbNISAhfT89ht/GNm7IJGHX8aPdxnHYbn506IWztudz1GyYvHzjBv73bhM1m4++L0vmrqalnXAmdGudkTWk+j79ew5o3annwMznMyUsKU6tFrGvYMf1Dhw6RmZlJRkYGTqeTuXPnsmvXrpBtamtrmTFjBgDTp09n9+7dAGRnZ5OVlQWAy+UiJSUFn8832sdw3px2Gw9+Jpui7AR+WHmM7R+1hrtJl6XDJ/w8/OpRNu1pZEZGPM/fPpnbr3Sdc+mL5FgnT5bkU+CK45/equPNI+H/WRCxmmFDv6WlBbfbPfjc7XbT0tISss3EiROprKwEoLKyku7ubtrb20O2OXToEIFAgIyMjNFo90WLcth5eF4Os7MSeO6Pxyg/3BbuJl02egIGW/Y28c3fHaGxo49v3pTNqvm5pCVEDbtvYrSDJxbkcVV6PN97u54y/cEVuaRG5UTuokWL2LRpE+Xl5RQWFuJyubDbh/6enDhxgueee4777rsv5PMnlZWVUVZWBsDatWvxeC58SqXT6Tyv/b93h5tlv6riB39owDUhhQVTI2c65/nWYiT21Lby9PZD1LT6ua0wnftvnkxK3PBhf7offN7NI7/Zz3N/PIYzNp47Z2WPajvPZizqcTlTPYZYqRbDhr7L5cLrHZri6PV6cblcZ2yzbNkyAPx+Pzt37hwct+/q6mLt2rXcddddTJs27azfo7S0lNLS0sHnzc3N538kAzwez3nv//BNGTyxo5fHf/cBXR2RM9Z8IbU4l46efv7lT4289lEbmYlRPLEgj9lZCfR1ttHceWFf88Eb0/luf4D15R/T0trOHdPdw+90EUazHpFA9RgSCbXIzh5Zx2nY4Z2CggIaGhpobGwkEAhQUVFBUVFRyDY+nw/DCN6x6qWXXqK4uBiAQCDAunXrmDdvHnPmzDnfY7hkYp12VhXncoU7lu/+vo7ddR3hbtK4YZomb1f7uP/lj9n+cRv/rdDFs5+bzOysiz8ZH+Ww89DNOdw8MYl/3dvEv/+5CdPUfRBExtKwPX2Hw8HixYtZs2YNhmFQXFxMXl4e27Zto6CggKKiIqqqqti6dSs2m43CwkKWLFkCQEVFBfv376e9vZ3y8nIA7rvvPiZNmjSWx3RB4qMcPFacx7e21/CdN+tYOT+Xa0Yh2C5n3q4+/nnXcXbWdjAlNYZVxaN/Va3TbuPrc7OJdhzjZ+956QmYfFlrJImMGZs5DrtW9fX1F7zvxb5Na+/pZ9X2aup8vayaf3lfSHShtTBMk1c+bOUne5sIGCZ3zfTwN58696yc0WCYJj/efZzfHmzltmkT+PuijFG/D0IkvIUfTarHkEioxagN71hNUoyDJxfkkZUYzVPltbzf2BXuJl1SNW09PPJaNS/uOs4V7lie/dxk7rjKPeZ3ILPbbNxTlMF/K3Tx24OtPP/HY7rlpcgYUOifRXA+eR5pCVE8+XqtJdaM6es3+dl7zfzjb49Q09bDA3Myg3/8kqIvWRtsNhtfviaNu672sP3jNtZX1Osm9yKjTKF/DhPigsGfGufgiddr+NAbucH/QVM33/jdYf79z83cmJfIC7dPoaRgQljG1W02G38708OXr0njraPtPP1WHX39xiVvh0ikUuh/And8FKtL8kmKcfCtHTV83OIPd5NGVVdfPz/adYzlrx6lq89g1fxcln0mhwlx4V+H746r3NxTlMHO2g7WvFFHT0DBLzIaFPrDSEuIYnVJHvFOO4/tqOFoa0+4mzQqdtV2cP/Lh4MnTq9M5bnbJ1OUM74WQfvclaksnZPJ3oZOniyvpauvP9xNErnsKfRHICMxmtWl+UTZbazaXk1t2+Ub/K3dAb77+zqeeqOWhCg7//TZidxTlEF81Pi8s1VpwQS+cVM2VY1dPL6jho5eBb/IxVDoj1BWUjSrS/OwAyu311Dv6w13k86LaZqUfdTKfS9/zB9rOrh7pofv/dVkrvTEhbtpw5o3KZmHbs7hoxY/q8qq8fkD4W6SyGVLoX8ecpNjeLI0H8MwWbm9mmPtl0fwN7T38tiOGp774zHyU2L4wW2T+B9Xe4hyXD4XQN2Yl8Sjt+RS6+vl0bJqWroV/CIXQqF/nvJTYniyJI/egMGq7dU0dvSFu0nnFDBMfvG+lwd+c5hDXj//cF0Ga27NJzclJtxNuyDXZieyan4ujZ19PPraUZo6x2/tRcYrhf4FmJQayxMl+XT2BYPf2zU+widgmNT7etld18Gv9rfw9z/by7/ubeKarASev30yfzUtddSvcr3UZmYm8PiCPFr9/Tzy2tHL5t2WyHihZRguwsHmbh7bXhO8K9St+bguwVRHwzRp6Q5Q7+ulztdLfXsv9QOPxzv66D/lfzMjKYavzHZzY15SxK1lc8jr5/Ed1UQ57KwuyRvRu5dIuNR+NKkeQyKhFiNdhkGhf5H2NwVnlaQlRPFUaT4TYkcn+H09/YNhPvg48O+eU5I92mEjOyma7ORospOiyRl4zE6OZkpOxmX/g/xJjrb28Nj2akwTnizJY1LqJy8GFwm/2KNJ9RgSCbVQ6F9C+4538cTrNWQlRfNUaT7JMSOb/ugPGDQMBHndqeHu66W9d+hiJLsNMhOjzgj3rKRo3PHOcw7ZRMIP8nBqfT08VlZDT7/B4wvymOo+92wkK9TjfKgeQyKhFgr9S+zdY508VV5LbnI0q0vySRwI/oBhcryj74zeel17L96u0Bko7nhnMNhP67FnJEbhvIAFzyLhB3kkjnf0smp7DT5/P48V53JVevxZt7NKPUZK9RgSCbVQ6IfBnvrgkgE5SdGkJTipb+/lWEcfp64ZlhhtDwn0nIHHrKRoYp2je149En6QR6q5q49VZTV4u/p45Jbcs97kxUr1GAnVY0gk1GKkoR/+RVYiyLXZiTx8czY/2nUcbzdMTo3lpvzkwSGZ7OToEQ/9yPnxxEfxnVvzeWxHDU+V17J8Xs64W1ZCZDxQ6I+y63OTuD43Mu6xe7mZEOfkqdJ8Ht9Rw3ferOWbN2UzNz853M0SGVc0T18iSnKMg9UleVzhiuO7v6+n/HBbuJs0bvgDBm9X+/ju7+v48n9+yCsfNIa7SRIG6ulLxEmIdvD4gjy+/UYt369ooCdg8tmpE8LdrLDo7jPYXddBRU0779R10NNvkhLrICXGwVOvHuTBz+jdkNUo9CUixUXZWTk/l396q44fVh6jt9/gq5/xhLtZl0RXXz+76zp5u9rHnvpOevtNUmMdLJiSwk0Tk7gqLZ7efpM1bzXwzNv1RDvsOv9hIZq9E8FUi+BtIJ95u44/1HRw65VpFKY6uCo9nszEqIi6Srmzt59ddR1UVLezp76TPsPEFefkxvwkbspL4lNpcWfc5zg2aQL3/sefqG7t5bHiXGZmnjnjySoi4XdFUzZFtRjQb5hs3NPIm0faae8JXhsxIdZBYVochWnxXJUex+TU2Au6FiKcOnr7qaztoKLax58auggYJu44J3Pzk7gpP4kr0+I+ca0lj8fDx7XHeLSsmsbOPh5fkEdh2tmvcYh0kfC7otAX1eI0LrebvR/Xs7+xm6qmLvY3dXN8YJXUGIeNKz1xFKbHcVVaPNM8sePyxjLtPf3srG2norqdd491EjAgLT4Y9HPzk5nmiR3xononfz5OdAd45LWjtPr7WV2SzxXuT17OIhJFwu+KQl9Ui9OcrR7erj72N3VT1dTNB01dHD7Rg2EGl76YNCGGwvR4rkqLozAtDnd8VFja7evpZ2fNUND3m5CeEDXYo5/qjr2goapT69HU2ccjrx2lu89gza0TmTjh8lx++0JFwu+KQl9Ui9OMpB5dff0cbPYH3wk0dnOguXtwgbuMxCgK04LvBArT48hNjh6zparb/AH+WBMcuvnz8S4MM7j+UrBHn8QVrgsL+lOdXo+G9l4eea0awzT5zq0TyU6OvtjDuGxEwu+KrsgVuQDxUQ5mZyUMLuMQMEwOn/BT1djN/qZu/tTQSflhHwBJ0XY+lRY/8IcgjivcsUSV3h/EAAAOzklEQVQ5LvzSl9buAH8Y6NHvawwGfVZSFHdc5eam/CQmp8aM6cnnrKRonizJ49HXqlm1vZrv3DqR9MTwvLuRsaOefgRTLUKNRj1M0+RYRx9VjV1UNQX/ENQN3C85ym5jqjs2+EcgPZ5PeeIGF947l5buAH+obqeipp2qgaDPSY7mpoEe/aQJYxf056rH4RN+Hi2rJinawbdvzQ/bsNalFAm/KxreEdXiNGNVjzZ/gP0DfwD2N3VxyOsfvJnNxJQYCtPjBmYKxZGeEBUM+pp23j7azv6mbkwgNzmamyYmcVN+Mvkp0ZdkOukn1ePAwA2CPPFOvn1rPimjdJ+I8SoSflcU+qJanOZS1aMnYPChd+i8wAfN3XT1Be+PkBLroM3fDwT/IMydGOzR54fhvsXD1eP94108/noNOcnRPHXKcuGRKBJ+VzSmLxImMU47MzLimZERnPPeb5hUt/Wwv6mbg83dZCdFMzc/adzfoH56RjyP3JLLU+W1PPF6DU+U5I3LaaxyfrTgmsgYc9htTE6N5bZpqfzj3Gy+cLVn3Af+SddkJfDQZ7I51OLnqfJaegLG8DtdRjp6+3lhZwN/97O91Pp6wt2cS0KhLyKf6Ia8JL4+N5uqxm6+/WYdff2REfy76zp44OXDlH3URm1rNw+9cpS9DZ3hbtaYU+iLyLDmTUrm/jmZ7G3o5Lu/rydgjLtTgSPW3tPP+op6VpfXkhjt4OnPTmTz3dfgiYviiddr+M2BE+Fu4phS6IvIiJQWTOCeogx21nbwg4oG+i/D4P9jTTtLX/6Yt474+MIMN8/81USmuuPISo5l7Wfz+XR2Ij/afZwXK49d1n/YPolO5IrIiH3uylT8AYOf7G0i2mnjvhsyx+yq5NHk8wf40e7jvHW0ncmpMTxWnMcUV+gaQ/FRDlbMy+Gn7zbxi6oW6ny9PHRzDkkRNmtJoS8i5+Xz0934Awb/sc9LjNPO3386fdwuU22aJm9Xt/OjXcfp7OvnizM93DHdfc4VVR12G1++Jp28lBhe2HmMB185wspbci+bE+8jodAXkfN290wPPQGDX31wgliHjUWz08Zd8Ld2B3hx1zH+UNPBFa5YVt+YP+KF5BZMSSErMYrvvFnHQ68c5cGbc7gmKzLuN6DQF5HzZrPZ+Oq16fT0m/xnVQuxTjtfuHp83JnMNE3eOOLj/+w+jj9g8j9np7Gw0HXGTWSGU5gez7q/nMRTb9Ty5Os1LPl0Op+bljru/ridL4W+iFwQm83G/7oug56Awb/9uZkYp52/KXSFtU3erj42VB5nV10HV3pieWBO1kUNzaQnRrH2L/JZX9HAj3c3Ut3ayz3XZVx2N9w5lUJfRC6Y3WZj6ZwsevtNNu1pJMZp4y+npl7ydpimyfaP29j0TiN9hsnia9O5/crU8+7dn83JE7xb9gZP8Na3B0/wJl+mJ3gV+iJyURx2G1+fm01PoJYXK48T47BTPCXlkn3/ps4+frjzGHsaOrkqLY6lc7JG/V4AdttpJ3j/6wir5l+eJ3g1T19ELlqUw8bD83K4OjOeZ//YQEW1b8y/p2mavPJhK0tfPkxVUxf3FGWw5tb8Mb35y4IpKTxVmkd3wOChV46yp75jzL7XWFHoi8ioiHbYefSWXK70xPHM2/Xsrhu7QDze0ctjO2r4YeUxprpjefZzk/nclamX5JqBwrR41n12EmkJUawur+XXH7QwDhcrPieFvoiMmlinnVXzc5k4IZa1b9bx52Oju5aNYZr85sAJHvjNYT5s9nPv9Zk8WZJHRuKlvbVj8ATvRK7LSeT/vNPIhsrjl80VvAp9ERlVCdEOHl+QR3ZSNGveqGV/U9eofN2G9l5WllXzo93HKUyL57nbJ/PZqRPCNoUyLsrO8nk5fP4qF68cauVbO2rw9fSHpS3nQ6EvIqMuOcbBEyV5uOKiePL1Wg55/Rf8tfoNk1/tb+GB3xzmyIkels7J5FvFuaQlhP82jnabjf95TTpfn5vFgaZuHvyvI9S0je8lmhX6IjImUuOcPFmSR2K0ncd3VHO09fzDsLathxWvVbNpTyOzMoO9+9KC8PXuz2X+5BSeKs2/LE7wKvRFZMykJUSxuiSfKIedx7ZXD95Efjj9hskv3vfyj789Qr2vh6/PzeLRW3LH9U3aP5UWxzN/OYmMxOAJ3v83Tk/wKvRFZExlJkWzuiQP04RV26s53vHJwX+0tYeHXz3Kv+5toigngedun8L8ySnjrnd/NmkJUXzn1uAJ3o3vNPLCzmP09Y+v4Ffoi8iYy02J4YmSPHoCBo9tr8Hb1XfGNgHD5D/ea+YbvzvM8Y4+HvxMNg/fnENq3OV1DenJE7x3Tnfz2kdtPL6jGp8/EO5mDVLoi8glMTk1lm8V59Hm7+ex7TW0nRKEH7f4WfZfR/i3PzczJy+J52+fzGcmJl8WvfuzsduCK49+fW4WB5r9PPjKUarHyQlehb6IXDLTPHGsKs6lsbOPb+2oobU7wL+928Sy/zrCie4Ay+fl8OBnckiJvbx69+cyf3IKa27Nxx8wePiVo7wzhhesjZRCX0Quqenp8Tx6Sy41bb0s+eVH/Mc+LzdPSub526dwY15SuJs36q70xLFu4ATvU2/U8qv94T3BO6I/p3v37mXz5s0YhkFJSQkLFy4Meb2pqYkNGzbg8/lITExk6dKluN1uAMrLy/nFL34BwB133MH8+fNH9whE5LIzOyuBh2/O5v/u8/KFGR6uy00Md5PGVFpC8Are9RX1bNrTSHVbD/9wXSZRjks/fDVs6BuGwcaNG1m5ciVut5sVK1ZQVFREbm7u4DZbtmxh3rx5zJ8/n3379rF161aWLl1KR0cHP//5z1m7di0Ay5cvp6ioiMTEyP4PFpHhXZ+bxPW5kdezP5dYp52Hb85h67vN/N/3vTS097L85hySL/FQ1rDDO4cOHSIzM5OMjAycTidz585l165dIdvU1tYyY8YMAKZPn87u3buB4DuEmTNnkpiYSGJiIjNnzmTv3r1jcBgiIuOf3WbjS7PT+MbcLA42+1n2ylGqL+CitYtqw3AbtLS0DA7VALjdblpaWkK2mThxIpWVlQBUVlbS3d1Ne3v7Gfu6XK4z9hURsZpbBk7w9g5cwTuWK5KeblTeVyxatIhNmzZRXl5OYWEhLpcLu33k54jLysooKysDYO3atXg8F36vTafTeVH7RxLVIpTqEUr1GBKOWng8sDEnneW/ruKp8lruv3ky/+Oa7DGfpjps6LtcLrxe7+Bzr9eLy+U6Y5tly5YB4Pf72blzJwkJCbhcLqqqqga3a2lp4aqrrjrje5SWllJaWjr4vLm5+fyPZIDH47mo/SOJahFK9QilegwJVy0cwOoFOXy/ooHn3jrM/voW7r0+84Ju85idnT2i7YbtjhcUFNDQ0EBjYyOBQICKigqKiopCtvH5fBiGAcBLL71EcXExALNnz+bdd9+lo6ODjo4O3n33XWbPnn2+xyIiErFinXYeujmbL8xwE+O0j8p9fT/JsD19h8PB4sWLWbNmDYZhUFxcTF5eHtu2baOgoICioiKqqqrYunUrNpuNwsJClixZAkBiYiKf//znWbFiBQB33nmnZu6IiJzGbrPxxVlpl2T+vs0ch8vA1dfXX/C+ess6RLUIpXqEUj2GREItRm14R0REIodCX0TEQhT6IiIWotAXEbEQhb6IiIUo9EVELEShLyJiIQp9ERELUeiLiFiIQl9ExEIU+iIiFqLQFxGxkHG54JqIiIyNiOvpL1++PNxNGDdUi1CqRyjVY4iVahFxoS8iIuem0BcRsZCIC/1T77VrdapFKNUjlOoxxEq10IlcERELibievoiInNuwN0a/XOzdu5fNmzdjGAYlJSUsXLgw3E0Km+bmZl544QVaW1ux2WyUlpZy2223hbtZYWUYBsuXL8flcllqpsbZdHZ28uKLL1JTU4PNZuNrX/sa06ZNC3ezwubll19mx44d2Gw28vLyuPfee4mOjg53s8ZMRIS+YRhs3LiRlStX4na7WbFiBUVFReTm5oa7aWHhcDhYtGgRU6ZMobu7m+XLlzNz5kzL1gPgt7/9LTk5OXR3d4e7KWG3efNmZs+ezTe/+U0CgQA9PT3hblLYtLS08Lvf/Y7169cTHR3N9773PSoqKpg/f364mzZmImJ459ChQ2RmZpKRkYHT6WTu3Lns2rUr3M0Km9TUVKZMmQJAXFwcOTk5tLS0hLlV4eP1etmzZw8lJSXhbkrYdXV1sX//fhYsWACA0+kkISEhzK0KL8Mw6O3tpb+/n97eXlJTU8PdpDEVET39lpYW3G734HO3282HH34YxhaNH42NjRw+fJgrrrgi3E0Jm3/5l3/hS1/6knr5BH8ekpOT+eEPf8jRo0eZMmUKX/nKV4iNjQ1308LC5XLx13/913zta18jOjqaWbNmMWvWrHA3a0xFRE9fzs7v9/PMM8/wla98hfj4+HA3JyzeeecdUlJSBt/5WF1/fz+HDx/mL/7iL3j66aeJiYnhl7/8ZbibFTYdHR3s2rWLF154gX/+53/G7/fz5ptvhrtZYyoiQt/lcuH1egefe71eXC5XGFsUfoFAgGeeeYabb76ZG264IdzNCZsDBw6we/du7rvvPr7//e+zb98+nn322XA3K2zcbjdut5upU6cCMGfOHA4fPhzmVoXPe++9R3p6OsnJyTidTm644QYOHjwY7maNqYgY3ikoKKChoYHGxkZcLhcVFRU88MAD4W5W2JimyYsvvkhOTg633357uJsTVnfffTd33303AO+//z6//vWvLf2zMWHCBNxuN/X19WRnZ/Pee+9Z+gS/x+Phww8/pKenh+joaN577z0KCgrC3awxFRGh73A4WLx4MWvWrMEwDIqLi8nLywt3s8LmwIEDvPnmm+Tn5/Pggw8CcNddd3HttdeGuWUyHixevJhnn32WQCBAeno69957b7ibFDZTp05lzpw5PPzwwzgcDiZNmhTxV+fqilwREQuJiDF9EREZGYW+iIiFKPRFRCxEoS8iYiEKfRERC1Hoi4hYiEJfRMRCFPoiIhby/wHIhFDIzwYtvQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(hist[\"acc\"])\n",
    "plt.plot(hist[\"val_acc\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Странно, что в первую эпоху на валидации результаты получились лучше. Возможно, что это из-за специфики примеров. В целом точность неплохая. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь сделаем предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_output_tags = []\n",
    "all_output_words = []\n",
    "for i in test.index:\n",
    "    p = model.predict(np.array([test_X[i]]))\n",
    "    p = np.argmax(p, axis=-1)\n",
    "    sent_words = []\n",
    "    sent_tags = []\n",
    "    for w, pred in zip(test_X[i], p[0]):\n",
    "        if test_words[w] != \"ENDPAD\":\n",
    "            sent_words.append(test_words[w])\n",
    "            sent_tags.append(test_tags[pred])\n",
    "    all_output_words.append(sent_words)\n",
    "    all_output_tags.append(sent_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['И', 'должен', 'ни', 'единой', 'долькой', 'Не', 'отступаться', 'от', 'лица', 'Но', 'быть', 'живым', 'живым', 'и', 'только', 'Живым', 'и', 'только', 'до', 'конца']\n"
     ]
    }
   ],
   "source": [
    "print(all_output_words[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-cV', 'O', 'O', 'O', 'O', 'B-cR1', 'B-cR1', 'I-cR1', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(all_output_tags[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Оценка результатов полной разметки (1 балл)\n",
    "Обученной сеткой разметьте тестовые данные, с помощью готового скрипта agrr_metrics.py оцените полученный результат.\n",
    "Пример вызова из командной строки:\n",
    "> python3 agrr_metrics.py test.csv output.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Анализ результатов (до 4 баллов)\n",
    "Проведите анализ ошибок. Посмотрите на fn, fp примеры, приведите несколько таких предложений в скобочной записи (если вы решали задачу классификации последовательности). Можно ли попытаться обобщить получаемые ошибки?\n",
    "\n",
    "to be continued... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
